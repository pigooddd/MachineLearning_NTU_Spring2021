{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 72us/sample - loss: 0.9048 - acc: 0.7621 - val_loss: 0.4837 - val_acc: 0.8776\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.3867 - acc: 0.8979 - val_loss: 0.3725 - val_acc: 0.8942\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.3114 - acc: 0.9159 - val_loss: 0.3316 - val_acc: 0.9065\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.2717 - acc: 0.9252 - val_loss: 0.2944 - val_acc: 0.9164\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.2439 - acc: 0.9325 - val_loss: 0.2906 - val_acc: 0.9141\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.2238 - acc: 0.9380 - val_loss: 0.2757 - val_acc: 0.9184\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.2053 - acc: 0.9430 - val_loss: 0.2947 - val_acc: 0.9092\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.1940 - acc: 0.9476 - val_loss: 0.2552 - val_acc: 0.9227\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.1783 - acc: 0.9510 - val_loss: 0.2560 - val_acc: 0.9197\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.1687 - acc: 0.9527 - val_loss: 0.2447 - val_acc: 0.9276\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1564 - acc: 0.9562 - val_loss: 0.2319 - val_acc: 0.9319\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1478 - acc: 0.9591 - val_loss: 0.2597 - val_acc: 0.9189\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.1421 - acc: 0.9630 - val_loss: 0.2279 - val_acc: 0.9304\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.1332 - acc: 0.9641 - val_loss: 0.2309 - val_acc: 0.9293\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.1265 - acc: 0.9663 - val_loss: 0.2177 - val_acc: 0.9337\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.1209 - acc: 0.9669 - val_loss: 0.2204 - val_acc: 0.9333\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.1139 - acc: 0.9707 - val_loss: 0.2243 - val_acc: 0.9306\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.1090 - acc: 0.9721 - val_loss: 0.2151 - val_acc: 0.9338\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1038 - acc: 0.9735 - val_loss: 0.2189 - val_acc: 0.9322\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0995 - acc: 0.9740 - val_loss: 0.2080 - val_acc: 0.9358\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0937 - acc: 0.9775 - val_loss: 0.2135 - val_acc: 0.9334\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0890 - acc: 0.9783 - val_loss: 0.2262 - val_acc: 0.9316\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0856 - acc: 0.9793 - val_loss: 0.2086 - val_acc: 0.9367\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0824 - acc: 0.9809 - val_loss: 0.2119 - val_acc: 0.9335\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0780 - acc: 0.9807 - val_loss: 0.2148 - val_acc: 0.9332\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0745 - acc: 0.9829 - val_loss: 0.2137 - val_acc: 0.9360\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0709 - acc: 0.9846 - val_loss: 0.2056 - val_acc: 0.9378\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0683 - acc: 0.9848 - val_loss: 0.2099 - val_acc: 0.9356\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0660 - acc: 0.9862 - val_loss: 0.2107 - val_acc: 0.9376\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0631 - acc: 0.9861 - val_loss: 0.2076 - val_acc: 0.9367\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0603 - acc: 0.9871 - val_loss: 0.2116 - val_acc: 0.9362\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0575 - acc: 0.9889 - val_loss: 0.2063 - val_acc: 0.9383\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0554 - acc: 0.9898 - val_loss: 0.2184 - val_acc: 0.9328\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0531 - acc: 0.9904 - val_loss: 0.2066 - val_acc: 0.9379\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0505 - acc: 0.9898 - val_loss: 0.2052 - val_acc: 0.9392\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0489 - acc: 0.9908 - val_loss: 0.2063 - val_acc: 0.9393\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0472 - acc: 0.9916 - val_loss: 0.2078 - val_acc: 0.9364\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0455 - acc: 0.9919 - val_loss: 0.2140 - val_acc: 0.9366\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0438 - acc: 0.9930 - val_loss: 0.2111 - val_acc: 0.9391\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0416 - acc: 0.9936 - val_loss: 0.2071 - val_acc: 0.9385\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0396 - acc: 0.9935 - val_loss: 0.2099 - val_acc: 0.9386\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0383 - acc: 0.9942 - val_loss: 0.2287 - val_acc: 0.9314\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0372 - acc: 0.9955 - val_loss: 0.2094 - val_acc: 0.9392\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0359 - acc: 0.9952 - val_loss: 0.2120 - val_acc: 0.9372\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0345 - acc: 0.9949 - val_loss: 0.2105 - val_acc: 0.9383\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0332 - acc: 0.9961 - val_loss: 0.2152 - val_acc: 0.9350\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0324 - acc: 0.9965 - val_loss: 0.2088 - val_acc: 0.9395\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0312 - acc: 0.9963 - val_loss: 0.2097 - val_acc: 0.9393\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0298 - acc: 0.9970 - val_loss: 0.2106 - val_acc: 0.9407\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0290 - acc: 0.9970 - val_loss: 0.2142 - val_acc: 0.9370\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0281 - acc: 0.9969 - val_loss: 0.2117 - val_acc: 0.9394\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0270 - acc: 0.9974 - val_loss: 0.2123 - val_acc: 0.9381\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0262 - acc: 0.9977 - val_loss: 0.2154 - val_acc: 0.9370\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0254 - acc: 0.9978 - val_loss: 0.2152 - val_acc: 0.9388\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0246 - acc: 0.9984 - val_loss: 0.2145 - val_acc: 0.9384\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0237 - acc: 0.9985 - val_loss: 0.2197 - val_acc: 0.9370\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0233 - acc: 0.9982 - val_loss: 0.2155 - val_acc: 0.9384\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0226 - acc: 0.9985 - val_loss: 0.2133 - val_acc: 0.9398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0221 - acc: 0.9985 - val_loss: 0.2130 - val_acc: 0.9394\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0214 - acc: 0.9985 - val_loss: 0.2167 - val_acc: 0.9388\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0206 - acc: 0.9985 - val_loss: 0.2182 - val_acc: 0.9381\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0201 - acc: 0.9991 - val_loss: 0.2170 - val_acc: 0.9389\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0195 - acc: 0.9987 - val_loss: 0.2173 - val_acc: 0.9387\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0189 - acc: 0.9989 - val_loss: 0.2171 - val_acc: 0.9381\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0186 - acc: 0.9988 - val_loss: 0.2181 - val_acc: 0.9383\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0180 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9388\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0176 - acc: 0.9992 - val_loss: 0.2202 - val_acc: 0.9390\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0171 - acc: 0.9992 - val_loss: 0.2225 - val_acc: 0.9379\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0169 - acc: 0.9991 - val_loss: 0.2209 - val_acc: 0.9387\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - acc: 0.9994 - val_loss: 0.2248 - val_acc: 0.9370\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0160 - acc: 0.9993 - val_loss: 0.2201 - val_acc: 0.9383\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - acc: 0.9994 - val_loss: 0.2202 - val_acc: 0.9376\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0151 - acc: 0.9994 - val_loss: 0.2229 - val_acc: 0.9378\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 1s 64us/sample - loss: 0.0149 - acc: 0.9995 - val_loss: 0.2211 - val_acc: 0.9382\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - acc: 0.9994 - val_loss: 0.2219 - val_acc: 0.9390\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0143 - acc: 0.9995 - val_loss: 0.2239 - val_acc: 0.9383\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0140 - acc: 0.9995 - val_loss: 0.2251 - val_acc: 0.9380\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0135 - acc: 0.9997 - val_loss: 0.2256 - val_acc: 0.9386\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0133 - acc: 0.9996 - val_loss: 0.2255 - val_acc: 0.9384\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0130 - acc: 0.9996 - val_loss: 0.2240 - val_acc: 0.9378\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0128 - acc: 0.9997 - val_loss: 0.2250 - val_acc: 0.9388\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0126 - acc: 0.9997 - val_loss: 0.2234 - val_acc: 0.9396\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0123 - acc: 0.9997 - val_loss: 0.2246 - val_acc: 0.9381\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0120 - acc: 0.9997 - val_loss: 0.2274 - val_acc: 0.9381\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0117 - acc: 0.9997 - val_loss: 0.2295 - val_acc: 0.9377\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0116 - acc: 0.9997 - val_loss: 0.2284 - val_acc: 0.9383\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0114 - acc: 0.9995 - val_loss: 0.2297 - val_acc: 0.9382\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0111 - acc: 0.9997 - val_loss: 0.2285 - val_acc: 0.9378\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0109 - acc: 0.9997 - val_loss: 0.2284 - val_acc: 0.9380\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0107 - acc: 0.9997 - val_loss: 0.2282 - val_acc: 0.9381\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0105 - acc: 0.9997 - val_loss: 0.2292 - val_acc: 0.9378\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0103 - acc: 0.9997 - val_loss: 0.2298 - val_acc: 0.9377\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0102 - acc: 0.9998 - val_loss: 0.2297 - val_acc: 0.9380\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0099 - acc: 0.9999 - val_loss: 0.2298 - val_acc: 0.9386\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0098 - acc: 0.9997 - val_loss: 0.2322 - val_acc: 0.9378\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0097 - acc: 0.9998 - val_loss: 0.2300 - val_acc: 0.9386\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0094 - acc: 0.9999 - val_loss: 0.2317 - val_acc: 0.9368\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0094 - acc: 0.9998 - val_loss: 0.2320 - val_acc: 0.9380\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0091 - acc: 0.9998 - val_loss: 0.2305 - val_acc: 0.9378\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0090 - acc: 0.9999 - val_loss: 0.2315 - val_acc: 0.9375\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b8996b1b1e0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "x_test = mnist['x_test']/255.\n",
    "y_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2 \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((784,), input_shape=(28,28)))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=1.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rec = model.fit(x_train, y_train, epochs=100, batch_size=120,\n",
    "                validation_data=(x_test, y_test))\n",
    "\n",
    "vep = np.linspace(1.,100.,100)\n",
    "fig = plt.figure(figsize=(6,6), dpi=80)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(vep,rec.history['loss'], lw=3)\n",
    "plt.plot(vep,rec.history['val_loss'], lw=3)\n",
    "plt.ylim(0.,0.35)\n",
    "plt.grid()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(vep,rec.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "x_test = mnist['x_test']/255.\n",
    "y_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Reshape((784,), input_shape=(28,28)))\n",
    "m1.add(Dense(30, activation='sigmoid'))\n",
    "m1.add(Dense(10, activation='softmax'))\n",
    "m1.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "m2 = Sequential()\n",
    "m2.add(Reshape((784,), input_shape=(28,28)))\n",
    "m2.add(Dense(30, activation='sigmoid'))\n",
    "m2.add(Dense(10, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "m2.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "rec1 = m1.fit(x_train, y_train, epochs=100, batch_size=120,\n",
    "              validation_data=(x_test, y_test))\n",
    "rec2 = m2.fit(x_train, y_train, epochs=100, batch_size=120,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "vep = np.linspace(1.,100.,100)\n",
    "fig = plt.figure(figsize=(6,6), dpi=80)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(vep,rec1.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec1.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(vep,rec2.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec2.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 91us/sample - loss: 0.9251 - acc: 0.7575 - val_loss: 0.5042 - val_acc: 0.8680\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.3958 - acc: 0.8954 - val_loss: 0.3851 - val_acc: 0.8884\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.3190 - acc: 0.9108 - val_loss: 0.3398 - val_acc: 0.9015\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.2766 - acc: 0.9225 - val_loss: 0.3154 - val_acc: 0.9096\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.2488 - acc: 0.9309 - val_loss: 0.2881 - val_acc: 0.9177\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.2260 - acc: 0.9367 - val_loss: 0.3070 - val_acc: 0.9062\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.2082 - acc: 0.9410 - val_loss: 0.2829 - val_acc: 0.9168\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.1939 - acc: 0.9470 - val_loss: 0.2643 - val_acc: 0.9228\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.1804 - acc: 0.9497 - val_loss: 0.2589 - val_acc: 0.9246\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.1677 - acc: 0.9546 - val_loss: 0.2441 - val_acc: 0.9271\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.1578 - acc: 0.9572 - val_loss: 0.2398 - val_acc: 0.9277\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.1493 - acc: 0.9601 - val_loss: 0.2381 - val_acc: 0.9271\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1399 - acc: 0.9629 - val_loss: 0.2318 - val_acc: 0.9296\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.1322 - acc: 0.9645 - val_loss: 0.2305 - val_acc: 0.9290\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.1263 - acc: 0.9656 - val_loss: 0.2466 - val_acc: 0.9241\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 1s 65us/sample - loss: 0.1200 - acc: 0.9681 - val_loss: 0.2307 - val_acc: 0.9278\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.1126 - acc: 0.9712 - val_loss: 0.2325 - val_acc: 0.9285\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.1089 - acc: 0.9712 - val_loss: 0.2202 - val_acc: 0.9318\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1024 - acc: 0.9751 - val_loss: 0.2260 - val_acc: 0.9300\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0974 - acc: 0.9754 - val_loss: 0.2283 - val_acc: 0.9315\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0941 - acc: 0.9774 - val_loss: 0.2264 - val_acc: 0.9314\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0894 - acc: 0.9786 - val_loss: 0.2156 - val_acc: 0.9339\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0856 - acc: 0.9798 - val_loss: 0.2413 - val_acc: 0.9276\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0809 - acc: 0.9798 - val_loss: 0.2296 - val_acc: 0.9299\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0788 - acc: 0.9815 - val_loss: 0.2382 - val_acc: 0.9258\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0757 - acc: 0.9834 - val_loss: 0.2427 - val_acc: 0.9258\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0727 - acc: 0.9833 - val_loss: 0.2160 - val_acc: 0.9345\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0696 - acc: 0.9846 - val_loss: 0.2152 - val_acc: 0.9330\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0660 - acc: 0.9856 - val_loss: 0.2226 - val_acc: 0.9327\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0633 - acc: 0.9874 - val_loss: 0.2159 - val_acc: 0.9319\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0611 - acc: 0.9877 - val_loss: 0.2190 - val_acc: 0.9340\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0586 - acc: 0.9882 - val_loss: 0.2183 - val_acc: 0.9343\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0563 - acc: 0.9889 - val_loss: 0.2190 - val_acc: 0.9345\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0543 - acc: 0.9902 - val_loss: 0.2216 - val_acc: 0.9330\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0525 - acc: 0.9905 - val_loss: 0.2466 - val_acc: 0.9259\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0513 - acc: 0.9906 - val_loss: 0.2166 - val_acc: 0.9345\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0481 - acc: 0.9923 - val_loss: 0.2207 - val_acc: 0.9351\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0461 - acc: 0.9925 - val_loss: 0.2223 - val_acc: 0.9320\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0446 - acc: 0.9926 - val_loss: 0.2200 - val_acc: 0.9343\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0429 - acc: 0.9932 - val_loss: 0.2233 - val_acc: 0.9337\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0413 - acc: 0.9933 - val_loss: 0.2201 - val_acc: 0.9338\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0405 - acc: 0.9937 - val_loss: 0.2257 - val_acc: 0.9337\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0389 - acc: 0.9940 - val_loss: 0.2226 - val_acc: 0.9351\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0371 - acc: 0.9946 - val_loss: 0.2225 - val_acc: 0.9347\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0357 - acc: 0.9953 - val_loss: 0.2259 - val_acc: 0.9349\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0349 - acc: 0.9954 - val_loss: 0.2243 - val_acc: 0.9351\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.0336 - acc: 0.9959 - val_loss: 0.2240 - val_acc: 0.9352\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0323 - acc: 0.9961 - val_loss: 0.2250 - val_acc: 0.9352\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0315 - acc: 0.9961 - val_loss: 0.2265 - val_acc: 0.9343\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0301 - acc: 0.9967 - val_loss: 0.2318 - val_acc: 0.9330\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0297 - acc: 0.9967 - val_loss: 0.2300 - val_acc: 0.9350\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0284 - acc: 0.9974 - val_loss: 0.2283 - val_acc: 0.9349\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0276 - acc: 0.9969 - val_loss: 0.2271 - val_acc: 0.9345\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.0265 - acc: 0.9980 - val_loss: 0.2286 - val_acc: 0.9351\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0257 - acc: 0.9973 - val_loss: 0.2277 - val_acc: 0.9341\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0249 - acc: 0.9982 - val_loss: 0.2315 - val_acc: 0.9349\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0245 - acc: 0.9978 - val_loss: 0.2350 - val_acc: 0.9342\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0239 - acc: 0.9980 - val_loss: 0.2347 - val_acc: 0.9340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0228 - acc: 0.9983 - val_loss: 0.2311 - val_acc: 0.9350\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0223 - acc: 0.9984 - val_loss: 0.2337 - val_acc: 0.9347\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0217 - acc: 0.9983 - val_loss: 0.2339 - val_acc: 0.9350\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0211 - acc: 0.9981 - val_loss: 0.2336 - val_acc: 0.9351\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0205 - acc: 0.9985 - val_loss: 0.2343 - val_acc: 0.9332\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0200 - acc: 0.9985 - val_loss: 0.2355 - val_acc: 0.9346\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0192 - acc: 0.9987 - val_loss: 0.2359 - val_acc: 0.9355\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0189 - acc: 0.9988 - val_loss: 0.2361 - val_acc: 0.9348\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0183 - acc: 0.9989 - val_loss: 0.2378 - val_acc: 0.9346\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0180 - acc: 0.9992 - val_loss: 0.2354 - val_acc: 0.9344\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0175 - acc: 0.9989 - val_loss: 0.2442 - val_acc: 0.9340\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0172 - acc: 0.9993 - val_loss: 0.2371 - val_acc: 0.9353\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0167 - acc: 0.9991 - val_loss: 0.2398 - val_acc: 0.9349\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0162 - acc: 0.9992 - val_loss: 0.2426 - val_acc: 0.9341\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0159 - acc: 0.9993 - val_loss: 0.2500 - val_acc: 0.9332\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0157 - acc: 0.9993 - val_loss: 0.2403 - val_acc: 0.9350\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0152 - acc: 0.9993 - val_loss: 0.2418 - val_acc: 0.9347\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.0148 - acc: 0.9994 - val_loss: 0.2460 - val_acc: 0.9345\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - acc: 0.9996 - val_loss: 0.2431 - val_acc: 0.9346\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0142 - acc: 0.9995 - val_loss: 0.2420 - val_acc: 0.9353\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0138 - acc: 0.9996 - val_loss: 0.2421 - val_acc: 0.9352\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0136 - acc: 0.9996 - val_loss: 0.2424 - val_acc: 0.9353\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0133 - acc: 0.9996 - val_loss: 0.2431 - val_acc: 0.9355\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0130 - acc: 0.9997 - val_loss: 0.2479 - val_acc: 0.9353\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0127 - acc: 0.9996 - val_loss: 0.2442 - val_acc: 0.9352\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0126 - acc: 0.9997 - val_loss: 0.2452 - val_acc: 0.9355\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0123 - acc: 0.9997 - val_loss: 0.2477 - val_acc: 0.9342\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0121 - acc: 0.9997 - val_loss: 0.2551 - val_acc: 0.9326\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0119 - acc: 0.9997 - val_loss: 0.2470 - val_acc: 0.9353\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0117 - acc: 0.9997 - val_loss: 0.2473 - val_acc: 0.9354\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0113 - acc: 0.9997 - val_loss: 0.2476 - val_acc: 0.9357\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0111 - acc: 0.9997 - val_loss: 0.2493 - val_acc: 0.9349\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0110 - acc: 0.9997 - val_loss: 0.2493 - val_acc: 0.9347\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0108 - acc: 0.9997 - val_loss: 0.2518 - val_acc: 0.9357\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0106 - acc: 0.9998 - val_loss: 0.2507 - val_acc: 0.9338\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.0102 - acc: 0.999 - 0s 35us/sample - loss: 0.0105 - acc: 0.9997 - val_loss: 0.2514 - val_acc: 0.9352\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0102 - acc: 0.9998 - val_loss: 0.2502 - val_acc: 0.9350\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0100 - acc: 0.9998 - val_loss: 0.2506 - val_acc: 0.9365\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0099 - acc: 1.0000 - val_loss: 0.2508 - val_acc: 0.9354\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0098 - acc: 0.9999 - val_loss: 0.2520 - val_acc: 0.9360\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0096 - acc: 0.9999 - val_loss: 0.2517 - val_acc: 0.9359\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2533 - val_acc: 0.9356\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 112us/sample - loss: 1.0620 - acc: 0.6870 - val_loss: 0.5238 - val_acc: 0.8692\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.5694 - acc: 0.8348 - val_loss: 0.4031 - val_acc: 0.8939\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.4756 - acc: 0.8586 - val_loss: 0.3391 - val_acc: 0.9067\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 1s 67us/sample - loss: 0.4371 - acc: 0.8684 - val_loss: 0.3210 - val_acc: 0.9094\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.4029 - acc: 0.8826 - val_loss: 0.3043 - val_acc: 0.9122\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.3749 - acc: 0.8887 - val_loss: 0.2946 - val_acc: 0.9126\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.3591 - acc: 0.8934 - val_loss: 0.2887 - val_acc: 0.9157\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.3459 - acc: 0.8996 - val_loss: 0.2568 - val_acc: 0.9260\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.3318 - acc: 0.9020 - val_loss: 0.2607 - val_acc: 0.9256\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.3213 - acc: 0.9025 - val_loss: 0.2458 - val_acc: 0.9289\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.3047 - acc: 0.9099 - val_loss: 0.2425 - val_acc: 0.9296\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.3019 - acc: 0.9094 - val_loss: 0.2376 - val_acc: 0.9313\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.3020 - acc: 0.9071 - val_loss: 0.2287 - val_acc: 0.9337\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.2850 - acc: 0.9127 - val_loss: 0.2306 - val_acc: 0.9334\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2749 - acc: 0.9175 - val_loss: 0.2243 - val_acc: 0.9328\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.2755 - acc: 0.9166 - val_loss: 0.2205 - val_acc: 0.9343\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2693 - acc: 0.9185 - val_loss: 0.2164 - val_acc: 0.9357\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.2666 - acc: 0.9170 - val_loss: 0.2143 - val_acc: 0.9363\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2614 - acc: 0.9184 - val_loss: 0.2258 - val_acc: 0.9343\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2555 - acc: 0.9230 - val_loss: 0.2128 - val_acc: 0.9376\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2496 - acc: 0.9203 - val_loss: 0.2066 - val_acc: 0.9393\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.2530 - acc: 0.9248 - val_loss: 0.2038 - val_acc: 0.9394\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2500 - acc: 0.9224 - val_loss: 0.2025 - val_acc: 0.9392\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2495 - acc: 0.9240 - val_loss: 0.2085 - val_acc: 0.9408\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.2406 - acc: 0.9242 - val_loss: 0.2036 - val_acc: 0.9410\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.2391 - acc: 0.9251 - val_loss: 0.2020 - val_acc: 0.9400\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.2342 - acc: 0.9263 - val_loss: 0.1989 - val_acc: 0.9394\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2336 - acc: 0.9277 - val_loss: 0.1978 - val_acc: 0.9413\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2355 - acc: 0.9270 - val_loss: 0.1975 - val_acc: 0.9421\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.2287 - acc: 0.9287 - val_loss: 0.2018 - val_acc: 0.9386\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.2309 - acc: 0.9307 - val_loss: 0.1969 - val_acc: 0.9404\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.2350 - acc: 0.9264 - val_loss: 0.1972 - val_acc: 0.9425\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.2214 - acc: 0.9337 - val_loss: 0.1982 - val_acc: 0.9403\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.2239 - acc: 0.9320 - val_loss: 0.1927 - val_acc: 0.9436\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.2206 - acc: 0.9289 - val_loss: 0.1966 - val_acc: 0.9419\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2286 - acc: 0.9277 - val_loss: 0.1949 - val_acc: 0.9430\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2177 - acc: 0.9333 - val_loss: 0.1957 - val_acc: 0.9421\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.2171 - acc: 0.9322 - val_loss: 0.1969 - val_acc: 0.9434\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.2190 - acc: 0.9310 - val_loss: 0.1917 - val_acc: 0.9461\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2183 - acc: 0.9326 - val_loss: 0.1981 - val_acc: 0.9422\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.2114 - acc: 0.9353 - val_loss: 0.1943 - val_acc: 0.9427\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2140 - acc: 0.9350 - val_loss: 0.1963 - val_acc: 0.9426\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2066 - acc: 0.9367 - val_loss: 0.1898 - val_acc: 0.9445\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2069 - acc: 0.9344 - val_loss: 0.2006 - val_acc: 0.9411\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2078 - acc: 0.9338 - val_loss: 0.1899 - val_acc: 0.9443\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2066 - acc: 0.9342 - val_loss: 0.1983 - val_acc: 0.9417\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 1s 64us/sample - loss: 0.2039 - acc: 0.9372 - val_loss: 0.1901 - val_acc: 0.9447\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1976 - acc: 0.9369 - val_loss: 0.1908 - val_acc: 0.9448\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.1984 - acc: 0.9372 - val_loss: 0.1912 - val_acc: 0.9451\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2017 - acc: 0.9351 - val_loss: 0.1893 - val_acc: 0.9458\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2027 - acc: 0.9367 - val_loss: 0.1915 - val_acc: 0.9440\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1955 - acc: 0.9368 - val_loss: 0.1949 - val_acc: 0.9446\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.2045 - acc: 0.9333 - val_loss: 0.1893 - val_acc: 0.9446\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 1s 67us/sample - loss: 0.1966 - acc: 0.9360 - val_loss: 0.1909 - val_acc: 0.9443\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1973 - acc: 0.9366 - val_loss: 0.1900 - val_acc: 0.9439\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 1s 64us/sample - loss: 0.2016 - acc: 0.9353 - val_loss: 0.1873 - val_acc: 0.9446\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 1s 50us/sample - loss: 0.2038 - acc: 0.9356 - val_loss: 0.1990 - val_acc: 0.9397\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.2023 - acc: 0.9361 - val_loss: 0.1909 - val_acc: 0.9435\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1987 - acc: 0.9377 - val_loss: 0.1962 - val_acc: 0.9442\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.1947 - acc: 0.9361 - val_loss: 0.1871 - val_acc: 0.9436\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1854 - acc: 0.9389 - val_loss: 0.1883 - val_acc: 0.9448\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1923 - acc: 0.9371 - val_loss: 0.1873 - val_acc: 0.9467\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.2021 - acc: 0.9362 - val_loss: 0.1925 - val_acc: 0.9428\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1921 - acc: 0.9394 - val_loss: 0.1876 - val_acc: 0.9454\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.1866 - acc: 0.9374 - val_loss: 0.1854 - val_acc: 0.9465\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 1s 62us/sample - loss: 0.1831 - acc: 0.9396 - val_loss: 0.1899 - val_acc: 0.9450\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.1860 - acc: 0.9409 - val_loss: 0.1916 - val_acc: 0.9454\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 1s 64us/sample - loss: 0.1823 - acc: 0.9401 - val_loss: 0.1857 - val_acc: 0.9453\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.1837 - acc: 0.9411 - val_loss: 0.1922 - val_acc: 0.9456\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 1s 64us/sample - loss: 0.1903 - acc: 0.9371 - val_loss: 0.1900 - val_acc: 0.9452\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1895 - acc: 0.9366 - val_loss: 0.1877 - val_acc: 0.9458\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1852 - acc: 0.9397 - val_loss: 0.1876 - val_acc: 0.9451\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.1873 - acc: 0.9403 - val_loss: 0.1865 - val_acc: 0.9455\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.1884 - acc: 0.9407 - val_loss: 0.1843 - val_acc: 0.9463\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1883 - acc: 0.9389 - val_loss: 0.1855 - val_acc: 0.9456\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.1834 - acc: 0.9433 - val_loss: 0.1939 - val_acc: 0.9449\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.1903 - acc: 0.9384 - val_loss: 0.1850 - val_acc: 0.9463\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.1805 - acc: 0.9412 - val_loss: 0.1933 - val_acc: 0.9424\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.1919 - acc: 0.9388 - val_loss: 0.1891 - val_acc: 0.9456\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1846 - acc: 0.9399 - val_loss: 0.1869 - val_acc: 0.9469\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1754 - acc: 0.9449 - val_loss: 0.1901 - val_acc: 0.9442\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.1880 - acc: 0.9392 - val_loss: 0.1862 - val_acc: 0.9467\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1814 - acc: 0.9430 - val_loss: 0.1941 - val_acc: 0.9443\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 1s 67us/sample - loss: 0.1695 - acc: 0.9478 - val_loss: 0.1838 - val_acc: 0.9471\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.1827 - acc: 0.9396 - val_loss: 0.1967 - val_acc: 0.9464\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.1870 - acc: 0.9398 - val_loss: 0.1876 - val_acc: 0.9451\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.1803 - acc: 0.9420 - val_loss: 0.1858 - val_acc: 0.9477\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.1821 - acc: 0.9402 - val_loss: 0.1922 - val_acc: 0.9450\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.1766 - acc: 0.9443 - val_loss: 0.1879 - val_acc: 0.9466\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.1784 - acc: 0.9439 - val_loss: 0.1883 - val_acc: 0.9468\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.1766 - acc: 0.9404 - val_loss: 0.1894 - val_acc: 0.9439\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.1728 - acc: 0.9430 - val_loss: 0.1858 - val_acc: 0.9473\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.1774 - acc: 0.9432 - val_loss: 0.1915 - val_acc: 0.9456\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.1776 - acc: 0.9411 - val_loss: 0.1915 - val_acc: 0.9458\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1751 - acc: 0.9438 - val_loss: 0.1857 - val_acc: 0.9478\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.1770 - acc: 0.9416 - val_loss: 0.1825 - val_acc: 0.9471\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1775 - acc: 0.9430 - val_loss: 0.1905 - val_acc: 0.9454\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1710 - acc: 0.9455 - val_loss: 0.1852 - val_acc: 0.9493\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 1s 63us/sample - loss: 0.1819 - acc: 0.9421 - val_loss: 0.1871 - val_acc: 0.9470\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1718 - acc: 0.9435 - val_loss: 0.1851 - val_acc: 0.9482\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-84bbf71b66e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "x_test = mnist['x_test']/255.\n",
    "y_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Reshape((784,), input_shape=(28,28)))\n",
    "m1.add(Dense(30, activation='sigmoid'))\n",
    "m1.add(Dense(10, activation='softmax'))\n",
    "m1.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "m2 = Sequential()\n",
    "m2.add(Reshape((784,), input_shape=(28,28)))\n",
    "m2.add(Dropout(0.2))\n",
    "m2.add(Dense(30, activation='sigmoid'))\n",
    "m2.add(Dropout(0.2))\n",
    "m2.add(Dense(10, activation='softmax'))\n",
    "m2.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "rec1 = m1.fit(x_train, y_train, epochs=100, batch_size=120,\n",
    "              validation_data=(x_test, y_test))\n",
    "rec2 = m2.fit(x_train, y_train, epochs=100, batch_size=120,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "vep = np.linspace(1.,100.,100)\n",
    "fig = plt.figure(figsize=(6,6), dpi=80)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(vep,rec1.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec1.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(vep,rec2.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec2.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 63us/sample - loss: 0.9117 - acc: 0.7670 - val_loss: 0.4651 - val_acc: 0.8758\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.3855 - acc: 0.8976 - val_loss: 0.3357 - val_acc: 0.9087\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.3078 - acc: 0.9142 - val_loss: 0.3008 - val_acc: 0.9136\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.2682 - acc: 0.9261 - val_loss: 0.2932 - val_acc: 0.9098\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.2406 - acc: 0.9317 - val_loss: 0.2705 - val_acc: 0.9192\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.2195 - acc: 0.9393 - val_loss: 0.2607 - val_acc: 0.9234\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.2043 - acc: 0.9423 - val_loss: 0.2752 - val_acc: 0.9169\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.1891 - acc: 0.9470 - val_loss: 0.2558 - val_acc: 0.9237\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.1775 - acc: 0.9507 - val_loss: 0.2334 - val_acc: 0.9313\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1663 - acc: 0.9537 - val_loss: 0.2306 - val_acc: 0.9317\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1564 - acc: 0.9578 - val_loss: 0.2219 - val_acc: 0.9334\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.1470 - acc: 0.9605 - val_loss: 0.2173 - val_acc: 0.9346\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1399 - acc: 0.9606 - val_loss: 0.2190 - val_acc: 0.9330\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.1327 - acc: 0.9642 - val_loss: 0.2089 - val_acc: 0.9379\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1252 - acc: 0.9680 - val_loss: 0.2104 - val_acc: 0.9350\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.1189 - acc: 0.9687 - val_loss: 0.2035 - val_acc: 0.9391\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1129 - acc: 0.9701 - val_loss: 0.2168 - val_acc: 0.9361\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.1067 - acc: 0.9728 - val_loss: 0.2009 - val_acc: 0.9414\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1023 - acc: 0.9754 - val_loss: 0.2022 - val_acc: 0.9392\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0985 - acc: 0.9745 - val_loss: 0.1993 - val_acc: 0.9400\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0927 - acc: 0.9767 - val_loss: 0.2067 - val_acc: 0.9367\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0897 - acc: 0.9776 - val_loss: 0.2234 - val_acc: 0.9309\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0852 - acc: 0.9802 - val_loss: 0.2019 - val_acc: 0.9390\n",
      "Performance (training)\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0837 - acc: 0.9826\n",
      "Loss: 0.08366, Acc: 0.98260\n",
      "Performance (validation)\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.2019 - acc: 0.9390\n",
      "Loss: 0.20193, Acc: 0.93900\n",
      "Performance (testing)\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.2171 - acc: 0.9346\n",
      "Loss: 0.21710, Acc: 0.93460\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "x_valid = mnist['x_train'][50000:]/255.\n",
    "y_valid = np.array([np.eye(10)[n] for n in mnist['y_train'][50000:]])\n",
    "x_test = mnist['x_test']/255.\n",
    "y_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((784,), input_shape=(28,28)))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "rec = model.fit(x_train, y_train, epochs=100, batch_size=120,\n",
    "                validation_data=(x_valid, y_valid),\n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "print('Performance (training)')\n",
    "print('Loss: %.5f, Acc: %.5f' % tuple(model.evaluate(x_train, y_train)))\n",
    "print('Performance (validation)')\n",
    "print('Loss: %.5f, Acc: %.5f' % tuple(model.evaluate(x_valid, y_valid)))\n",
    "print('Performance (testing)')\n",
    "print('Loss: %.5f, Acc: %.5f' % tuple(model.evaluate(x_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 74us/sample - loss: 0.9426 - acc: 0.7537 - val_loss: 0.4878 - val_acc: 0.8772\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.3884 - acc: 0.8978 - val_loss: 0.3830 - val_acc: 0.8872\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.3110 - acc: 0.9159 - val_loss: 0.3287 - val_acc: 0.9047\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.2711 - acc: 0.9243 - val_loss: 0.3679 - val_acc: 0.8908\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.2431 - acc: 0.9297 - val_loss: 0.3010 - val_acc: 0.9116\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.2207 - acc: 0.9373 - val_loss: 0.2778 - val_acc: 0.9206\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.2026 - acc: 0.9417 - val_loss: 0.2580 - val_acc: 0.9246\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1856 - acc: 0.9469 - val_loss: 0.2666 - val_acc: 0.9202\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1736 - acc: 0.9492 - val_loss: 0.2569 - val_acc: 0.9270\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.1618 - acc: 0.9541 - val_loss: 0.2730 - val_acc: 0.9171\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.1509 - acc: 0.9572 - val_loss: 0.2409 - val_acc: 0.9292\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.1411 - acc: 0.9605 - val_loss: 0.2379 - val_acc: 0.9304\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.1351 - acc: 0.9614 - val_loss: 0.2459 - val_acc: 0.9262\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.1263 - acc: 0.9650 - val_loss: 0.2435 - val_acc: 0.9257\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.1202 - acc: 0.9669 - val_loss: 0.2304 - val_acc: 0.9316\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.1131 - acc: 0.9690 - val_loss: 0.2356 - val_acc: 0.9293\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1082 - acc: 0.9705 - val_loss: 0.2205 - val_acc: 0.9358\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1022 - acc: 0.9738 - val_loss: 0.2112 - val_acc: 0.9358\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0972 - acc: 0.9741 - val_loss: 0.2154 - val_acc: 0.9363\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0917 - acc: 0.9766 - val_loss: 0.2146 - val_acc: 0.9368\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0878 - acc: 0.9775 - val_loss: 0.2274 - val_acc: 0.9332\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0841 - acc: 0.9792 - val_loss: 0.2236 - val_acc: 0.9330\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0796 - acc: 0.9809 - val_loss: 0.2135 - val_acc: 0.9374\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0756 - acc: 0.9822 - val_loss: 0.2122 - val_acc: 0.9374\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0726 - acc: 0.9840 - val_loss: 0.2070 - val_acc: 0.9373\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0700 - acc: 0.9840 - val_loss: 0.2042 - val_acc: 0.9396\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0667 - acc: 0.9859 - val_loss: 0.2051 - val_acc: 0.9391\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0639 - acc: 0.9870 - val_loss: 0.2071 - val_acc: 0.9384\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0611 - acc: 0.9874 - val_loss: 0.2057 - val_acc: 0.9395\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0585 - acc: 0.9882 - val_loss: 0.2107 - val_acc: 0.9393\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0563 - acc: 0.9896 - val_loss: 0.2187 - val_acc: 0.9365\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0541 - acc: 0.9902 - val_loss: 0.2120 - val_acc: 0.9382\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0522 - acc: 0.9912 - val_loss: 0.2100 - val_acc: 0.9376\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0496 - acc: 0.9910 - val_loss: 0.2069 - val_acc: 0.9375\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0483 - acc: 0.9918 - val_loss: 0.2164 - val_acc: 0.9358\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0462 - acc: 0.9924 - val_loss: 0.2054 - val_acc: 0.9400\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0441 - acc: 0.9934 - val_loss: 0.2122 - val_acc: 0.9374\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0427 - acc: 0.9936 - val_loss: 0.2059 - val_acc: 0.9390\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0410 - acc: 0.9940 - val_loss: 0.2060 - val_acc: 0.9398\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0390 - acc: 0.9948 - val_loss: 0.2103 - val_acc: 0.9385\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0377 - acc: 0.9948 - val_loss: 0.2141 - val_acc: 0.9367\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0369 - acc: 0.9951 - val_loss: 0.2083 - val_acc: 0.9388\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0354 - acc: 0.9957 - val_loss: 0.2101 - val_acc: 0.9392\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0346 - acc: 0.9951 - val_loss: 0.2701 - val_acc: 0.9204\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.0335 - acc: 0.9961 - val_loss: 0.2116 - val_acc: 0.9383\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0322 - acc: 0.9961 - val_loss: 0.2098 - val_acc: 0.9396\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0312 - acc: 0.9967 - val_loss: 0.2071 - val_acc: 0.9392\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0298 - acc: 0.9973 - val_loss: 0.2098 - val_acc: 0.9413\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0289 - acc: 0.9972 - val_loss: 0.2101 - val_acc: 0.9393\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0282 - acc: 0.9974 - val_loss: 0.2159 - val_acc: 0.9384\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0272 - acc: 0.9974 - val_loss: 0.2136 - val_acc: 0.9399\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0266 - acc: 0.9981 - val_loss: 0.2121 - val_acc: 0.9393\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0255 - acc: 0.9977 - val_loss: 0.2122 - val_acc: 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0247 - acc: 0.9980 - val_loss: 0.2134 - val_acc: 0.9408\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0239 - acc: 0.9982 - val_loss: 0.2118 - val_acc: 0.9395\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0233 - acc: 0.9985 - val_loss: 0.2114 - val_acc: 0.9394\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0227 - acc: 0.9982 - val_loss: 0.2120 - val_acc: 0.9395\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0220 - acc: 0.9988 - val_loss: 0.2203 - val_acc: 0.9384\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0217 - acc: 0.9987 - val_loss: 0.2131 - val_acc: 0.9393\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0209 - acc: 0.9989 - val_loss: 0.2153 - val_acc: 0.9377\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0204 - acc: 0.9988 - val_loss: 0.2168 - val_acc: 0.9390\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0199 - acc: 0.9988 - val_loss: 0.2157 - val_acc: 0.9386\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0194 - acc: 0.9990 - val_loss: 0.2166 - val_acc: 0.9385\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0188 - acc: 0.9989 - val_loss: 0.2170 - val_acc: 0.9387\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0182 - acc: 0.9990 - val_loss: 0.2157 - val_acc: 0.9402\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0180 - acc: 0.9989 - val_loss: 0.2171 - val_acc: 0.9395\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0174 - acc: 0.9991 - val_loss: 0.2182 - val_acc: 0.9394\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0171 - acc: 0.9993 - val_loss: 0.2192 - val_acc: 0.9394\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0166 - acc: 0.9992 - val_loss: 0.2185 - val_acc: 0.9393\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0164 - acc: 0.9993 - val_loss: 0.2191 - val_acc: 0.9399\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0160 - acc: 0.9993 - val_loss: 0.2249 - val_acc: 0.9382\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0157 - acc: 0.9994 - val_loss: 0.2211 - val_acc: 0.9390\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0153 - acc: 0.9994 - val_loss: 0.2199 - val_acc: 0.9392\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0148 - acc: 0.9994 - val_loss: 0.2262 - val_acc: 0.9384\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0145 - acc: 0.9996 - val_loss: 0.2211 - val_acc: 0.9392\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - acc: 0.9995 - val_loss: 0.2223 - val_acc: 0.9394\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0140 - acc: 0.9994 - val_loss: 0.2249 - val_acc: 0.9385\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0137 - acc: 0.9997 - val_loss: 0.2214 - val_acc: 0.9389\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0135 - acc: 0.9995 - val_loss: 0.2231 - val_acc: 0.9390\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0132 - acc: 0.9996 - val_loss: 0.2254 - val_acc: 0.9389\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0129 - acc: 0.9996 - val_loss: 0.2237 - val_acc: 0.9399\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0126 - acc: 0.9997 - val_loss: 0.2237 - val_acc: 0.9396\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0123 - acc: 0.9998 - val_loss: 0.2239 - val_acc: 0.9398\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0122 - acc: 0.9996 - val_loss: 0.2249 - val_acc: 0.9396\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0118 - acc: 0.9997 - val_loss: 0.2277 - val_acc: 0.9385\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0117 - acc: 0.9997 - val_loss: 0.2257 - val_acc: 0.9393\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0115 - acc: 0.9998 - val_loss: 0.2244 - val_acc: 0.9401\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0113 - acc: 0.9998 - val_loss: 0.2273 - val_acc: 0.9391\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0111 - acc: 0.9997 - val_loss: 0.2270 - val_acc: 0.9396\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0109 - acc: 0.9997 - val_loss: 0.2278 - val_acc: 0.9391\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0107 - acc: 0.9998 - val_loss: 0.2275 - val_acc: 0.9390\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0105 - acc: 0.9998 - val_loss: 0.2272 - val_acc: 0.9399\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0104 - acc: 0.9998 - val_loss: 0.2272 - val_acc: 0.9392\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0102 - acc: 0.9998 - val_loss: 0.2293 - val_acc: 0.9391\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0100 - acc: 0.9998 - val_loss: 0.2278 - val_acc: 0.9398\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0099 - acc: 0.9998 - val_loss: 0.2306 - val_acc: 0.9403\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0097 - acc: 0.9998 - val_loss: 0.2287 - val_acc: 0.9391\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0096 - acc: 0.9998 - val_loss: 0.2289 - val_acc: 0.9392\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0094 - acc: 0.9998 - val_loss: 0.2301 - val_acc: 0.9399\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0093 - acc: 0.9998 - val_loss: 0.2321 - val_acc: 0.9392\n",
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "30000/30000 [==============================] - 1s 33us/sample - loss: 0.6871 - acc: 0.8062 - val_loss: 0.3394 - val_acc: 0.9022\n",
      "Epoch 2/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.3711 - acc: 0.8964 - val_loss: 0.2727 - val_acc: 0.9202\n",
      "Epoch 3/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.3121 - acc: 0.9111 - val_loss: 0.2476 - val_acc: 0.9262\n",
      "Epoch 4/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.2726 - acc: 0.9213 - val_loss: 0.2341 - val_acc: 0.9300\n",
      "Epoch 5/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.2467 - acc: 0.9284 - val_loss: 0.2186 - val_acc: 0.9329\n",
      "Epoch 6/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.2237 - acc: 0.9341 - val_loss: 0.2058 - val_acc: 0.9384\n",
      "Epoch 7/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.2075 - acc: 0.9401 - val_loss: 0.1903 - val_acc: 0.9391\n",
      "Epoch 8/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.1936 - acc: 0.9440 - val_loss: 0.1874 - val_acc: 0.9420\n",
      "Epoch 9/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.1815 - acc: 0.9470 - val_loss: 0.1843 - val_acc: 0.9432\n",
      "Epoch 10/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.1707 - acc: 0.9510 - val_loss: 0.1776 - val_acc: 0.9439\n",
      "Epoch 11/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.1621 - acc: 0.9524 - val_loss: 0.1788 - val_acc: 0.9438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.1548 - acc: 0.9549 - val_loss: 0.1732 - val_acc: 0.9472\n",
      "Epoch 13/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.1479 - acc: 0.9574 - val_loss: 0.1785 - val_acc: 0.9454\n",
      "Epoch 14/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.1410 - acc: 0.9592 - val_loss: 0.1773 - val_acc: 0.9449\n",
      "Epoch 15/100\n",
      "30000/30000 [==============================] - 1s 25us/sample - loss: 0.1361 - acc: 0.9604 - val_loss: 0.1686 - val_acc: 0.9474\n",
      "Epoch 16/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.1303 - acc: 0.9629 - val_loss: 0.1726 - val_acc: 0.9446\n",
      "Epoch 17/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.1248 - acc: 0.9643 - val_loss: 0.1680 - val_acc: 0.9460\n",
      "Epoch 18/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.1206 - acc: 0.9648 - val_loss: 0.1681 - val_acc: 0.9464\n",
      "Epoch 19/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.1167 - acc: 0.9675 - val_loss: 0.1680 - val_acc: 0.9468\n",
      "Epoch 20/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.1131 - acc: 0.9686 - val_loss: 0.1674 - val_acc: 0.9476\n",
      "Epoch 21/100\n",
      "30000/30000 [==============================] - 1s 24us/sample - loss: 0.1087 - acc: 0.9695 - val_loss: 0.1732 - val_acc: 0.9460\n",
      "Epoch 22/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.1050 - acc: 0.9703 - val_loss: 0.1649 - val_acc: 0.9498\n",
      "Epoch 23/100\n",
      "30000/30000 [==============================] - 1s 28us/sample - loss: 0.1014 - acc: 0.9719 - val_loss: 0.1717 - val_acc: 0.9479\n",
      "Epoch 24/100\n",
      "30000/30000 [==============================] - 1s 28us/sample - loss: 0.0991 - acc: 0.9724 - val_loss: 0.1765 - val_acc: 0.9454\n",
      "Epoch 25/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0968 - acc: 0.9733 - val_loss: 0.1737 - val_acc: 0.9468\n",
      "Epoch 26/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0941 - acc: 0.9732 - val_loss: 0.1728 - val_acc: 0.9454\n",
      "Epoch 27/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0908 - acc: 0.9744 - val_loss: 0.1676 - val_acc: 0.9482\n",
      "Epoch 28/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0884 - acc: 0.9765 - val_loss: 0.1825 - val_acc: 0.9437\n",
      "Epoch 29/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0858 - acc: 0.9772 - val_loss: 0.1685 - val_acc: 0.9482\n",
      "Epoch 30/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0839 - acc: 0.9780 - val_loss: 0.1678 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0818 - acc: 0.9776 - val_loss: 0.1675 - val_acc: 0.9481\n",
      "Epoch 32/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0794 - acc: 0.9797 - val_loss: 0.1701 - val_acc: 0.9485\n",
      "Epoch 33/100\n",
      "30000/30000 [==============================] - 1s 23us/sample - loss: 0.0769 - acc: 0.9794 - val_loss: 0.1758 - val_acc: 0.9463\n",
      "Epoch 34/100\n",
      "30000/30000 [==============================] - 1s 28us/sample - loss: 0.0752 - acc: 0.9801 - val_loss: 0.1768 - val_acc: 0.9465\n",
      "Epoch 35/100\n",
      "30000/30000 [==============================] - 1s 24us/sample - loss: 0.0737 - acc: 0.9805 - val_loss: 0.1751 - val_acc: 0.9456\n",
      "Epoch 36/100\n",
      "30000/30000 [==============================] - 1s 23us/sample - loss: 0.0706 - acc: 0.9820 - val_loss: 0.1781 - val_acc: 0.9458\n",
      "Epoch 37/100\n",
      "30000/30000 [==============================] - 1s 26us/sample - loss: 0.0702 - acc: 0.9824 - val_loss: 0.1787 - val_acc: 0.9478\n",
      "Epoch 38/100\n",
      "30000/30000 [==============================] - 1s 24us/sample - loss: 0.0682 - acc: 0.9822 - val_loss: 0.1787 - val_acc: 0.9457\n",
      "Epoch 39/100\n",
      "30000/30000 [==============================] - 1s 25us/sample - loss: 0.0667 - acc: 0.9824 - val_loss: 0.1778 - val_acc: 0.9471\n",
      "Epoch 40/100\n",
      "30000/30000 [==============================] - 1s 23us/sample - loss: 0.0643 - acc: 0.9834 - val_loss: 0.1800 - val_acc: 0.9465\n",
      "Epoch 41/100\n",
      "30000/30000 [==============================] - 1s 23us/sample - loss: 0.0626 - acc: 0.9839 - val_loss: 0.1793 - val_acc: 0.9479\n",
      "Epoch 42/100\n",
      "30000/30000 [==============================] - 1s 27us/sample - loss: 0.0618 - acc: 0.9842 - val_loss: 0.1858 - val_acc: 0.9455\n",
      "Epoch 43/100\n",
      "30000/30000 [==============================] - 1s 30us/sample - loss: 0.0609 - acc: 0.9850 - val_loss: 0.1845 - val_acc: 0.9467\n",
      "Epoch 44/100\n",
      "30000/30000 [==============================] - 1s 24us/sample - loss: 0.0593 - acc: 0.9859 - val_loss: 0.1850 - val_acc: 0.9462\n",
      "Epoch 45/100\n",
      "30000/30000 [==============================] - 1s 27us/sample - loss: 0.0584 - acc: 0.9855 - val_loss: 0.1873 - val_acc: 0.9462\n",
      "Epoch 46/100\n",
      "30000/30000 [==============================] - 1s 24us/sample - loss: 0.0561 - acc: 0.9867 - val_loss: 0.1833 - val_acc: 0.9480\n",
      "Epoch 47/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0553 - acc: 0.9864 - val_loss: 0.1850 - val_acc: 0.9477\n",
      "Epoch 48/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0540 - acc: 0.9875 - val_loss: 0.1863 - val_acc: 0.9460\n",
      "Epoch 49/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0526 - acc: 0.9877 - val_loss: 0.1881 - val_acc: 0.9443\n",
      "Epoch 50/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0514 - acc: 0.9881 - val_loss: 0.1919 - val_acc: 0.9455\n",
      "Epoch 51/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0507 - acc: 0.9881 - val_loss: 0.1867 - val_acc: 0.9477\n",
      "Epoch 52/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0494 - acc: 0.9889 - val_loss: 0.1875 - val_acc: 0.9461\n",
      "Epoch 53/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0484 - acc: 0.9888 - val_loss: 0.1923 - val_acc: 0.9465\n",
      "Epoch 54/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0474 - acc: 0.9892 - val_loss: 0.1991 - val_acc: 0.9434\n",
      "Epoch 55/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0464 - acc: 0.9897 - val_loss: 0.1929 - val_acc: 0.9461\n",
      "Epoch 56/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0454 - acc: 0.9898 - val_loss: 0.1938 - val_acc: 0.9461\n",
      "Epoch 57/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0446 - acc: 0.9901 - val_loss: 0.2006 - val_acc: 0.9435\n",
      "Epoch 58/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0437 - acc: 0.9901 - val_loss: 0.1985 - val_acc: 0.9452\n",
      "Epoch 59/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0427 - acc: 0.9910 - val_loss: 0.2019 - val_acc: 0.9440\n",
      "Epoch 60/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0416 - acc: 0.9911 - val_loss: 0.2004 - val_acc: 0.9431\n",
      "Epoch 61/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0408 - acc: 0.9910 - val_loss: 0.2045 - val_acc: 0.9430\n",
      "Epoch 62/100\n",
      "30000/30000 [==============================] - 1s 25us/sample - loss: 0.0406 - acc: 0.9912 - val_loss: 0.2048 - val_acc: 0.9423\n",
      "Epoch 63/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0394 - acc: 0.9918 - val_loss: 0.2069 - val_acc: 0.9411\n",
      "Epoch 64/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0386 - acc: 0.9913 - val_loss: 0.2030 - val_acc: 0.9461\n",
      "Epoch 65/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0379 - acc: 0.9924 - val_loss: 0.2067 - val_acc: 0.9441\n",
      "Epoch 66/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0368 - acc: 0.9928 - val_loss: 0.2083 - val_acc: 0.9433\n",
      "Epoch 67/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0362 - acc: 0.9931 - val_loss: 0.2024 - val_acc: 0.9449\n",
      "Epoch 68/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0357 - acc: 0.9931 - val_loss: 0.2121 - val_acc: 0.9414\n",
      "Epoch 69/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0353 - acc: 0.9932 - val_loss: 0.2139 - val_acc: 0.9421\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 1s 36us/sample - loss: 0.0339 - acc: 0.9932 - val_loss: 0.2087 - val_acc: 0.9428\n",
      "Epoch 71/100\n",
      "30000/30000 [==============================] - 1s 23us/sample - loss: 0.0339 - acc: 0.9935 - val_loss: 0.2105 - val_acc: 0.9421\n",
      "Epoch 72/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0327 - acc: 0.9940 - val_loss: 0.2099 - val_acc: 0.9436\n",
      "Epoch 73/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0330 - acc: 0.9937 - val_loss: 0.2126 - val_acc: 0.9423\n",
      "Epoch 74/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0320 - acc: 0.9941 - val_loss: 0.2173 - val_acc: 0.9417\n",
      "Epoch 75/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0309 - acc: 0.9944 - val_loss: 0.2110 - val_acc: 0.9442\n",
      "Epoch 76/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0305 - acc: 0.9945 - val_loss: 0.2130 - val_acc: 0.9428\n",
      "Epoch 77/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0306 - acc: 0.9947 - val_loss: 0.2164 - val_acc: 0.9418\n",
      "Epoch 78/100\n",
      "30000/30000 [==============================] - 1s 19us/sample - loss: 0.0292 - acc: 0.9950 - val_loss: 0.2156 - val_acc: 0.9434\n",
      "Epoch 79/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0291 - acc: 0.9951 - val_loss: 0.2172 - val_acc: 0.9422\n",
      "Epoch 80/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0283 - acc: 0.9952 - val_loss: 0.2207 - val_acc: 0.9424\n",
      "Epoch 81/100\n",
      "30000/30000 [==============================] - 1s 19us/sample - loss: 0.0276 - acc: 0.9957 - val_loss: 0.2196 - val_acc: 0.9421\n",
      "Epoch 82/100\n",
      "30000/30000 [==============================] - 1s 23us/sample - loss: 0.0273 - acc: 0.9955 - val_loss: 0.2202 - val_acc: 0.9416\n",
      "Epoch 83/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0270 - acc: 0.9957 - val_loss: 0.2234 - val_acc: 0.9408\n",
      "Epoch 84/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0263 - acc: 0.9963 - val_loss: 0.2234 - val_acc: 0.9409\n",
      "Epoch 85/100\n",
      "30000/30000 [==============================] - 1s 19us/sample - loss: 0.0259 - acc: 0.9961 - val_loss: 0.2194 - val_acc: 0.9414\n",
      "Epoch 86/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0254 - acc: 0.9963 - val_loss: 0.2189 - val_acc: 0.9418\n",
      "Epoch 87/100\n",
      "30000/30000 [==============================] - 1s 23us/sample - loss: 0.0249 - acc: 0.9961 - val_loss: 0.2238 - val_acc: 0.9420\n",
      "Epoch 88/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0245 - acc: 0.9965 - val_loss: 0.2240 - val_acc: 0.9408\n",
      "Epoch 89/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0243 - acc: 0.9969 - val_loss: 0.2277 - val_acc: 0.9401\n",
      "Epoch 90/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0238 - acc: 0.9968 - val_loss: 0.2263 - val_acc: 0.9402\n",
      "Epoch 91/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0234 - acc: 0.9969 - val_loss: 0.2301 - val_acc: 0.9395\n",
      "Epoch 92/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0228 - acc: 0.9969 - val_loss: 0.2299 - val_acc: 0.9411\n",
      "Epoch 93/100\n",
      "30000/30000 [==============================] - 1s 20us/sample - loss: 0.0226 - acc: 0.9968 - val_loss: 0.2342 - val_acc: 0.9396\n",
      "Epoch 94/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0223 - acc: 0.9971 - val_loss: 0.2301 - val_acc: 0.9401\n",
      "Epoch 95/100\n",
      "30000/30000 [==============================] - 1s 26us/sample - loss: 0.0219 - acc: 0.9973 - val_loss: 0.2336 - val_acc: 0.9409\n",
      "Epoch 96/100\n",
      "30000/30000 [==============================] - 1s 32us/sample - loss: 0.0215 - acc: 0.9972 - val_loss: 0.2363 - val_acc: 0.9396\n",
      "Epoch 97/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0214 - acc: 0.9971 - val_loss: 0.2334 - val_acc: 0.9399\n",
      "Epoch 98/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0206 - acc: 0.9976 - val_loss: 0.2316 - val_acc: 0.9413\n",
      "Epoch 99/100\n",
      "30000/30000 [==============================] - 1s 21us/sample - loss: 0.0204 - acc: 0.9978 - val_loss: 0.2387 - val_acc: 0.9394\n",
      "Epoch 100/100\n",
      "30000/30000 [==============================] - 1s 22us/sample - loss: 0.0199 - acc: 0.9979 - val_loss: 0.2370 - val_acc: 0.9399\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d23a45f8fea5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "x_test = mnist['x_test']/255.\n",
    "y_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n",
    "\n",
    "from skimage.transform import rotate\n",
    "ext1 = np.array([rotate(img,np.random.uniform(+5.,+25.)) for img in x_train])\n",
    "ext2 = np.array([rotate(img,np.random.uniform(-25.,-5.)) for img in x_train])\n",
    "x_train_ext = np.vstack([x_train,ext1,ext2])\n",
    "y_train_ext = np.vstack([y_train,y_train,y_train])\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Reshape((784,), input_shape=(28,28)))\n",
    "m1.add(Dense(30, activation='sigmoid'))\n",
    "m1.add(Dense(10, activation='softmax'))\n",
    "m1.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "m2 = clone_model(m1)\n",
    "m2.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "rec1 = m1.fit(x_train, y_train, epochs=100, batch_size=120,\n",
    "              validation_data=(x_test, y_test))\n",
    "rec2 = m2.fit(x_train_ext, y_train_ext, epochs=100, batch_size=120,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "vep = np.linspace(1.,100.,100)\n",
    "fig = plt.figure(figsize=(6,6), dpi=80)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(vep,rec1.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec1.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(vep,rec2.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec2.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 103us/sample - loss: 1.3125\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.6169\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.45320s - loss: 0.456\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.3827\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.3412\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.3128\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.2925\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.2753\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2612\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.2494\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.2382\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.2289\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.2199\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.2116\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.2050\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.1984\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1919\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.1853\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.1802\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.1751\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.1693\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1652\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.1604\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.1564\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1521\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.1483\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1444\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1410\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.13670s - loss: 0.135\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1337\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.1304\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1276\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1248\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1221\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.1198\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1165\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.11430s - loss: 0.115\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.1117\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1092\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1065\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1045\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.1023\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.1003\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0982\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0960\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0939\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0920\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0905\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0884\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0866\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0848\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0831\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0819\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0801\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0787\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0768\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0755\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0739\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0726\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0717\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0701\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0690\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0675\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0664\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0653\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0638\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0627\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0618\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0606\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0594\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0585\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0577\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0567\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0557\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0545\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0535\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0528\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0520\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0512\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0503\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0495\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0486\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0477\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0472\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0464\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.04570s - loss: 0.\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0449\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0443\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0435\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0429\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0421\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0414\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0409\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0403\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0398\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0391\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0384\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0379\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0373\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0367\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 76us/sample - loss: 0.6237\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2794\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.2176\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.1801\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.1526\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 1s 61us/sample - loss: 0.1319\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.1138\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0999\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0898\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0767\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0686\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0585\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0552\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0497\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.0415\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.0393\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.0332\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0308\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0273\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0241\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0233\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0207\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0188\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0173\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0163\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0150\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0139\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0130\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0119\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0112\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0105\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0099\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0094\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0090\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0085\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0081\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0076\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0073\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0070\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0067\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0064\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0062\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0059\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0058\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0055\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0053\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0052\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0050\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0048\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0047\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0045\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0044\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.0043\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0042\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0041\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0040\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0038\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0037\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0036\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0036\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0035\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0034\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0033\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0032\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0032\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0031\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0030\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0030\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0029\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0029\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0028\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0027\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0027\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0026\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0026\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0025\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.0025\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.00240s - loss: 0.0\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0024\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0023\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0023\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0023\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0022\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0022\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0022\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0021\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0021\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0021\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0020\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.00200s - loss: 0.0\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0020\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0019\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0019\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0019\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0019\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0018\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0018\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0018\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0018\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0017\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 80us/sample - loss: 4.6708\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 2.5312\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 2.4672\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 2.5127\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 2.3955\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 2.3742\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 2.3154\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 2.3137\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 2.0754\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 1.9675\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 2.0115\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.9616\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 1.9219\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 1.8963\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 1.8738\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 1s 63us/sample - loss: 1.7879\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.7771\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 1.6893\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.7504\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.7238\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.6861\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.4139\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.2403\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.2107\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.1568\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1863\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 1.1355\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.1800\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1401\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1648\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.1383\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.165 - 0s 26us/sample - loss: 1.1564\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1219\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1175\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1578\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.0858\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1122\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1016\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.0598\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1091\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1283\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1301\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1034\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0841\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0833\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.07610s - loss: 1.06\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1148\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 1.0767\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0872\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1014\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0718\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.0348\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0981\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 1.1138\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 1.0932\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 1.0809\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 1.1046\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 1.0815\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.0583\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0957\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1274\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0781\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.1094\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.05360s - loss: 1.054\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 1.0815\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.1136\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.0083\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 1.1256\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0600\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0329\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 1.0694\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 1.0581\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 1.0611\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 1.0511\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.9556\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.8708\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.8656\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.8500\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.8607\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.8514\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.8521\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.8559\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.8642\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.8071\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.8196\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.7905\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.8061\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.7975\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.8133\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.7957\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.8172\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.8356\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.8001\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.8284\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.8204\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.80350s - loss: 0.80\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.8027\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.7876\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.78450s - loss: 0.7\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.8168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Reshape((784,), input_shape=(28,28)))\n",
    "m1.add(Dense(30, activation='sigmoid'))\n",
    "m1.add(Dense(10, activation='softmax'))\n",
    "m1.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=0.2))\n",
    "\n",
    "m2 = clone_model(m1)\n",
    "m2.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=2.0))\n",
    "\n",
    "m3 = clone_model(m1)\n",
    "m3.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=20.))\n",
    "\n",
    "rec1 = m1.fit(x_train, y_train, epochs=100, batch_size=60)\n",
    "rec2 = m2.fit(x_train, y_train, epochs=100, batch_size=60)\n",
    "rec3 = m3.fit(x_train, y_train, epochs=100, batch_size=60)\n",
    "\n",
    "vep = np.linspace(1.,100.,100)\n",
    "fig = plt.figure(figsize=(6,6), dpi=80)\n",
    "plt.plot(vep,rec1.history['loss'], lw=3)\n",
    "plt.plot(vep,rec2.history['loss'], lw=3)\n",
    "plt.plot(vep,rec3.history['loss'], lw=3)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 99us/sample - loss: 0.6760\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.3119\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.2545\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.2175\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1891\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.1676\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.1496\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.1368\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1226\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1125\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.1041\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0960\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0893\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0809\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0740\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0680\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0644\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0584\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0542\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.05010s - loss: 0.050\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0466\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0432\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0391\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0377\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0346\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0322\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0307\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0286\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0264\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0251\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0236\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0225\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.02160s - loss: 0\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0200\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0189\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0180\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0172\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0163\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0142\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0138\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0130\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0126\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0121\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0116\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0113\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0108\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0103\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0100\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0097\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0093\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0091\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0088\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0086\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0084\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0081\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0079\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0077\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0075\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0073\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0070\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0069\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0067\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0066\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0064\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0063\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0061\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0060\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0058\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0058\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0056\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0055\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0054\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0052\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0052\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0050\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0050\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0049\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.0048\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0047\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0046\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0045\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0044\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0044\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0043\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0042\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0041\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0041\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0040\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0039\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0039\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0038\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0038\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0037\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0037\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0036\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0036\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.00350s - loss: 0\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0035\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.5746\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.2707\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.2076\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1764\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1504\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1316\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.1139\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1021\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0892\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0791\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0713\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0628\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.05430s - loss: 0.05\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0497\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0444\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0403\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0357\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.03260s - loss: 0.03\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0288\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0260\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0245\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0225\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0204\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0186\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0177\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0160\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0145\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0138\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0129\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0119\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0113\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0108\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.01010s - loss: 0.\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0096\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.00920s - loss: 0.0 - ETA: 0s - loss: 0.00\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0086\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0083\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0080\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0076\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0073\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0070\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0067\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0065\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0062\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0060\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0059\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0056\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0054\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0053\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0051\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0050\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0048\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0046\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0045\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0044\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0042\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0041\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0040\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0039\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0038\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0037\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0036\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0035\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0034\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0034\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0033\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0032\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0031\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0031\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0030\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0029\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0029\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0028\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0028\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0027\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 0.0027\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0026\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0026\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0025\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0025\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0024\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0024\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0024\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0023\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0023\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0023\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.0022\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0022\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.0021\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0021\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0021\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0021\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0020\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0020\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.00200s - loss: 0\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.001 - 0s 29us/sample - loss: 0.0019\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0019\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0019\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0019\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Reshape((784,), input_shape=(28,28)))\n",
    "m1.add(Dense(30, activation='sigmoid'))\n",
    "m1.add(Dense(10, activation='softmax'))\n",
    "m1.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0))\n",
    "\n",
    "m2 = clone_model(m1)\n",
    "m2.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0, momentum=0.4))\n",
    "\n",
    "rec1 = m1.fit(x_train, y_train, epochs=100, batch_size=60)\n",
    "rec2 = m2.fit(x_train, y_train, epochs=100, batch_size=60)\n",
    "\n",
    "vep = np.linspace(1.,100.,100)\n",
    "fig = plt.figure(figsize=(6,6), dpi=80)\n",
    "plt.plot(vep,rec1.history['loss'], lw=3)\n",
    "plt.plot(vep,rec2.history['loss'], lw=3)\n",
    "plt.ylim(-0.05,0.3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 2.1372 - acc: 0.2720 - val_loss: 1.3121 - val_acc: 0.5553\n",
      "Epoch 2/40\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.7438 - acc: 0.7532 - val_loss: 0.6092 - val_acc: 0.7985\n",
      "Epoch 3/40\n",
      "10000/10000 [==============================] - 1s 107us/sample - loss: 0.4237 - acc: 0.8696 - val_loss: 0.4255 - val_acc: 0.8706\n",
      "Epoch 4/40\n",
      "10000/10000 [==============================] - 1s 103us/sample - loss: 0.3244 - acc: 0.9015 - val_loss: 0.3433 - val_acc: 0.8962\n",
      "Epoch 5/40\n",
      "10000/10000 [==============================] - 1s 105us/sample - loss: 0.2780 - acc: 0.9186 - val_loss: 0.3735 - val_acc: 0.8840\n",
      "Epoch 6/40\n",
      "10000/10000 [==============================] - 1s 93us/sample - loss: 0.2458 - acc: 0.9231 - val_loss: 0.3017 - val_acc: 0.9080\n",
      "Epoch 7/40\n",
      "10000/10000 [==============================] - 1s 89us/sample - loss: 0.2158 - acc: 0.9339 - val_loss: 0.2593 - val_acc: 0.9213\n",
      "Epoch 8/40\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 0.1881 - acc: 0.9419 - val_loss: 0.2597 - val_acc: 0.9198\n",
      "Epoch 9/40\n",
      "10000/10000 [==============================] - 1s 84us/sample - loss: 0.1689 - acc: 0.9477 - val_loss: 0.2330 - val_acc: 0.9309\n",
      "Epoch 10/40\n",
      "10000/10000 [==============================] - 1s 93us/sample - loss: 0.1551 - acc: 0.9534 - val_loss: 0.2264 - val_acc: 0.9314\n",
      "Epoch 11/40\n",
      "10000/10000 [==============================] - 1s 113us/sample - loss: 0.1385 - acc: 0.9584 - val_loss: 0.2609 - val_acc: 0.9212\n",
      "Epoch 12/40\n",
      "10000/10000 [==============================] - 1s 110us/sample - loss: 0.1227 - acc: 0.9635 - val_loss: 0.2072 - val_acc: 0.9366\n",
      "Epoch 13/40\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 0.1096 - acc: 0.9680 - val_loss: 0.2445 - val_acc: 0.9276\n",
      "Epoch 14/40\n",
      "10000/10000 [==============================] - 1s 94us/sample - loss: 0.1012 - acc: 0.9684 - val_loss: 0.2263 - val_acc: 0.9319\n",
      "Epoch 15/40\n",
      "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0913 - acc: 0.9718 - val_loss: 0.1915 - val_acc: 0.9417\n",
      "Epoch 16/40\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.0806 - acc: 0.9768 - val_loss: 0.1984 - val_acc: 0.9401\n",
      "Epoch 17/40\n",
      "10000/10000 [==============================] - 1s 94us/sample - loss: 0.0720 - acc: 0.9798 - val_loss: 0.2006 - val_acc: 0.9395\n",
      "Epoch 18/40\n",
      "10000/10000 [==============================] - 1s 92us/sample - loss: 0.0625 - acc: 0.9825 - val_loss: 0.1854 - val_acc: 0.9454\n",
      "Epoch 19/40\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0547 - acc: 0.9851 - val_loss: 0.1822 - val_acc: 0.9454\n",
      "Epoch 20/40\n",
      "10000/10000 [==============================] - 1s 101us/sample - loss: 0.0479 - acc: 0.9874 - val_loss: 0.1831 - val_acc: 0.9468\n",
      "Epoch 21/40\n",
      "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0422 - acc: 0.9893 - val_loss: 0.1832 - val_acc: 0.9475\n",
      "Epoch 22/40\n",
      "10000/10000 [==============================] - 1s 109us/sample - loss: 0.0390 - acc: 0.9911 - val_loss: 0.1760 - val_acc: 0.9492\n",
      "Epoch 23/40\n",
      "10000/10000 [==============================] - 1s 104us/sample - loss: 0.0328 - acc: 0.9921 - val_loss: 0.1935 - val_acc: 0.9426\n",
      "Epoch 24/40\n",
      "10000/10000 [==============================] - 1s 111us/sample - loss: 0.0289 - acc: 0.9935 - val_loss: 0.1913 - val_acc: 0.9470\n",
      "Epoch 25/40\n",
      "10000/10000 [==============================] - 1s 116us/sample - loss: 0.0244 - acc: 0.9957 - val_loss: 0.1824 - val_acc: 0.9497\n",
      "Epoch 26/40\n",
      "10000/10000 [==============================] - 1s 90us/sample - loss: 0.0218 - acc: 0.9958 - val_loss: 0.1905 - val_acc: 0.9475\n",
      "Epoch 27/40\n",
      "10000/10000 [==============================] - 1s 117us/sample - loss: 0.0186 - acc: 0.9970 - val_loss: 0.1820 - val_acc: 0.9510\n",
      "Epoch 28/40\n",
      "10000/10000 [==============================] - 1s 112us/sample - loss: 0.0165 - acc: 0.9970 - val_loss: 0.1998 - val_acc: 0.9480\n",
      "Epoch 29/40\n",
      "10000/10000 [==============================] - 1s 90us/sample - loss: 0.0144 - acc: 0.9983 - val_loss: 0.1837 - val_acc: 0.9509\n",
      "Epoch 30/40\n",
      "10000/10000 [==============================] - 1s 89us/sample - loss: 0.0127 - acc: 0.9983 - val_loss: 0.1904 - val_acc: 0.9489\n",
      "Epoch 31/40\n",
      "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0107 - acc: 0.9990 - val_loss: 0.1834 - val_acc: 0.9529\n",
      "Epoch 32/40\n",
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.0093 - acc: 0.9993 - val_loss: 0.1846 - val_acc: 0.9516\n",
      "Epoch 33/40\n",
      "10000/10000 [==============================] - 1s 89us/sample - loss: 0.0082 - acc: 0.9997 - val_loss: 0.1866 - val_acc: 0.9517\n",
      "Epoch 34/40\n",
      "10000/10000 [==============================] - 1s 84us/sample - loss: 0.0073 - acc: 0.9996 - val_loss: 0.1891 - val_acc: 0.9503\n",
      "Epoch 35/40\n",
      "10000/10000 [==============================] - 1s 83us/sample - loss: 0.0066 - acc: 0.9995 - val_loss: 0.1875 - val_acc: 0.9529\n",
      "Epoch 36/40\n",
      "10000/10000 [==============================] - 1s 84us/sample - loss: 0.0059 - acc: 0.9998 - val_loss: 0.1908 - val_acc: 0.9518\n",
      "Epoch 37/40\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 0.0054 - acc: 0.9998 - val_loss: 0.1888 - val_acc: 0.9522\n",
      "Epoch 38/40\n",
      "10000/10000 [==============================] - 1s 94us/sample - loss: 0.0050 - acc: 0.9999 - val_loss: 0.1917 - val_acc: 0.9514\n",
      "Epoch 39/40\n",
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.0046 - acc: 0.9999 - val_loss: 0.1906 - val_acc: 0.9525\n",
      "Epoch 40/40\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 0.1905 - val_acc: 0.9521\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "10000/10000 [==============================] - 2s 161us/sample - loss: 0.5742 - acc: 0.8328 - val_loss: 0.2960 - val_acc: 0.9110\n",
      "Epoch 2/40\n",
      "10000/10000 [==============================] - 1s 112us/sample - loss: 0.2380 - acc: 0.9305 - val_loss: 0.2362 - val_acc: 0.9286\n",
      "Epoch 3/40\n",
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.1669 - acc: 0.9520 - val_loss: 0.2580 - val_acc: 0.9201\n",
      "Epoch 4/40\n",
      "10000/10000 [==============================] - 1s 86us/sample - loss: 0.1271 - acc: 0.9607 - val_loss: 0.1768 - val_acc: 0.9444\n",
      "Epoch 5/40\n",
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.0986 - acc: 0.9706 - val_loss: 0.1697 - val_acc: 0.9481\n",
      "Epoch 6/40\n",
      "10000/10000 [==============================] - 1s 89us/sample - loss: 0.0713 - acc: 0.9816 - val_loss: 0.1491 - val_acc: 0.9524\n",
      "Epoch 7/40\n",
      "10000/10000 [==============================] - 1s 104us/sample - loss: 0.0529 - acc: 0.9858 - val_loss: 0.1696 - val_acc: 0.9474\n",
      "Epoch 8/40\n",
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.0417 - acc: 0.9891 - val_loss: 0.1455 - val_acc: 0.9539\n",
      "Epoch 9/40\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 0.0291 - acc: 0.9943 - val_loss: 0.1574 - val_acc: 0.9526\n",
      "Epoch 10/40\n",
      "10000/10000 [==============================] - 1s 86us/sample - loss: 0.0207 - acc: 0.9963 - val_loss: 0.1547 - val_acc: 0.9536\n",
      "Epoch 11/40\n",
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.0154 - acc: 0.9976 - val_loss: 0.1445 - val_acc: 0.9575\n",
      "Epoch 12/40\n",
      "10000/10000 [==============================] - 1s 89us/sample - loss: 0.0113 - acc: 0.9987 - val_loss: 0.1502 - val_acc: 0.9568\n",
      "Epoch 13/40\n",
      "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0087 - acc: 0.9994 - val_loss: 0.1548 - val_acc: 0.9572\n",
      "Epoch 14/40\n",
      "10000/10000 [==============================] - 1s 86us/sample - loss: 0.0061 - acc: 0.9999 - val_loss: 0.1538 - val_acc: 0.9565\n",
      "Epoch 15/40\n",
      "10000/10000 [==============================] - 1s 91us/sample - loss: 0.0049 - acc: 0.9999 - val_loss: 0.1550 - val_acc: 0.9582\n",
      "Epoch 16/40\n",
      "10000/10000 [==============================] - 1s 107us/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 0.1533 - val_acc: 0.9587\n",
      "Epoch 17/40\n",
      "10000/10000 [==============================] - 1s 81us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.1550 - val_acc: 0.9589\n",
      "Epoch 18/40\n",
      "10000/10000 [==============================] - 1s 83us/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40\n",
      "10000/10000 [==============================] - 1s 130us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.1570 - val_acc: 0.9582\n",
      "Epoch 20/40\n",
      "10000/10000 [==============================] - 1s 105us/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 0.1587 - val_acc: 0.9589\n",
      "Epoch 21/40\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1606 - val_acc: 0.9586\n",
      "Epoch 22/40\n",
      "10000/10000 [==============================] - 1s 92us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1619 - val_acc: 0.9590\n",
      "Epoch 23/40\n",
      "10000/10000 [==============================] - 1s 125us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1625 - val_acc: 0.9589\n",
      "Epoch 24/40\n",
      "10000/10000 [==============================] - 1s 107us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.1647 - val_acc: 0.9590\n",
      "Epoch 25/40\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1637 - val_acc: 0.9587\n",
      "Epoch 26/40\n",
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1648 - val_acc: 0.9587\n",
      "Epoch 27/40\n",
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1654 - val_acc: 0.9587\n",
      "Epoch 28/40\n",
      "10000/10000 [==============================] - 1s 84us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1657 - val_acc: 0.9595\n",
      "Epoch 29/40\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1672 - val_acc: 0.9592\n",
      "Epoch 30/40\n",
      "10000/10000 [==============================] - 1s 84us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1686 - val_acc: 0.9594\n",
      "Epoch 31/40\n",
      "10000/10000 [==============================] - 1s 83us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1693 - val_acc: 0.9597\n",
      "Epoch 32/40\n",
      "10000/10000 [==============================] - 1s 84us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1689 - val_acc: 0.9592\n",
      "Epoch 33/40\n",
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.1703 - val_acc: 0.9592\n",
      "Epoch 34/40\n",
      "10000/10000 [==============================] - 1s 82us/sample - loss: 9.5656e-04 - acc: 1.0000 - val_loss: 0.1703 - val_acc: 0.9589\n",
      "Epoch 35/40\n",
      "10000/10000 [==============================] - 1s 83us/sample - loss: 9.1604e-04 - acc: 1.0000 - val_loss: 0.1714 - val_acc: 0.9589\n",
      "Epoch 36/40\n",
      "10000/10000 [==============================] - 1s 99us/sample - loss: 8.7193e-04 - acc: 1.0000 - val_loss: 0.1715 - val_acc: 0.9590\n",
      "Epoch 37/40\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 8.4259e-04 - acc: 1.0000 - val_loss: 0.1726 - val_acc: 0.9589\n",
      "Epoch 38/40\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 8.0529e-04 - acc: 1.0000 - val_loss: 0.1738 - val_acc: 0.9590ETA: 0s - loss: 8.0537e-04 - acc: \n",
      "Epoch 39/40\n",
      "10000/10000 [==============================] - 1s 83us/sample - loss: 7.7557e-04 - acc: 1.0000 - val_loss: 0.1739 - val_acc: 0.9592\n",
      "Epoch 40/40\n",
      "10000/10000 [==============================] - 1s 82us/sample - loss: 7.4623e-04 - acc: 1.0000 - val_loss: 0.1749 - val_acc: 0.9594\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-7857232dc823>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train'][:10000]/255.\n",
    "y_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\n",
    "x_test = mnist['x_test']/255.\n",
    "y_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Reshape((784,), input_shape=(28,28)))\n",
    "m1.add(Dense(256, activation='sigmoid'))\n",
    "m1.add(Dense(256, activation='sigmoid'))\n",
    "m1.add(Dense(10, activation='softmax'))\n",
    "m1.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=1.0), metrics=['accuracy'])\n",
    "\n",
    "m2 = Sequential()\n",
    "m2.add(Reshape((784,), input_shape=(28,28)))\n",
    "m2.add(Dense(256, activation='relu'))\n",
    "m2.add(Dense(256, activation='relu'))\n",
    "m2.add(Dense(10, activation='softmax'))\n",
    "m2.compile(loss='categorical_crossentropy',\n",
    "           optimizer=SGD(lr=0.2), metrics=['accuracy'])\n",
    "\n",
    "rec1 = m1.fit(x_train, y_train, epochs=40, batch_size=60,\n",
    "              validation_data=(x_test, y_test))\n",
    "rec2 = m2.fit(x_train, y_train, epochs=40, batch_size=60,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "vep = np.linspace(1.,40.,40)\n",
    "fig = plt.figure(figsize=(6,6), dpi=80)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(vep,rec1.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec1.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(vep,rec2.history['accuracy'], lw=3)\n",
    "plt.plot(vep,rec2.history['val_accuracy'], lw=3)\n",
    "plt.ylim(0.85,1.01)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([[[np.random.randn()for m in range(14)]for n in range(14)]for i in range(100)])\n",
    "y_train=np.array([np.random.randn() for n in range(100)])\n",
    "for n,m in enumerate(y_train):\n",
    "    if m<0:\n",
    "        y_train[n]=0\n",
    "#     elif m>-0.2 and m<=0:\n",
    "#         y_train[n]=1\n",
    "#     elif m>0 and m<=0.2:\n",
    "#         y_train[n]=2\n",
    "    else:\n",
    "        y_train[n]=1\n",
    "W1=np.zeros((196,20), dtype=np.int)\n",
    "W2=np.zeros((20,1), dtype=np.int)\n",
    "# x_test=np.array([[[np.random.randn()for m in range(14)]for n in range(14)]for i in range(200)])\n",
    "\n",
    "x_test=np.array([[[np.random.randn()for m in range(14)]for n in range(14)]for i in range(100)])\n",
    "y_test=np.array([np.random.randn() for n in range(100)])\n",
    "for n,m in enumerate(y_test):\n",
    "    if m<0:\n",
    "        y_test[n]=0\n",
    "#     elif m>-0.2 and m<=0:\n",
    "#         y_train[n]=1\n",
    "#     elif m>0 and m<=0.2:\n",
    "#         y_train[n]=2\n",
    "    else:\n",
    "        y_test[n]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def loss_vs_epoch(x_train, y_train, x_test, y_test, W1, W2):\n",
    "    loss = np.zeros((2,60))\n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Reshape((196,), input_shape=(14,14)))\n",
    "    model.add(Dense(units=20, activation='sigmoid'))\n",
    "    model.add(Dense(units=1, activation='sigmoid',kernel_regularizer=l2(0.1)))   \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.0))\n",
    "    model.layers[1].set_weights([W1,np.zeros(20)])\n",
    "    model.layers[2].set_weights([W2,np.zeros(1)])\n",
    "\n",
    "    for i in range(60):\n",
    "        rec=model.fit(x_train, y_train, epochs=1, batch_size=100)#,validation_data=(x_test, y_test))\n",
    "        loss[0][i] = model.evaluate(x_train, y_train)\n",
    "        loss[1][i] = model.evaluate(x_test, y_test)\n",
    "    \n",
    "#     training_loss=rec.history['loss']\n",
    "#     testing_loss=rec.history['val_loss']\n",
    "#     y_predict=model.predict(x_test)\n",
    "    \n",
    "#     loss[0]=training_loss\n",
    "#     loss[1]=testing_loss\n",
    "    #### END YOUR CODE HERE ####\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 9ms/sample - loss: 0.6931\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 0.6906\n",
      "100/100 [==============================] - 0s 120us/sample - loss: 0.7086\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6906\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6893\n",
      "100/100 [==============================] - 0s 81us/sample - loss: 0.6956\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.6893\n",
      "100/100 [==============================] - 0s 101us/sample - loss: 0.6887\n",
      "100/100 [==============================] - 0s 119us/sample - loss: 0.7035\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.6887\n",
      "100/100 [==============================] - 0s 69us/sample - loss: 0.6883\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6979\n",
      "100/100 [==============================] - 0s 121us/sample - loss: 0.6883\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.6881\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.7017\n",
      "100/100 [==============================] - 0s 59us/sample - loss: 0.6881\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6879\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6993\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.6879\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6878\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7012\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6878\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6876\n",
      "100/100 [==============================] - 0s 130us/sample - loss: 0.7002\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6876\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6875\n",
      "100/100 [==============================] - 0s 110us/sample - loss: 0.7011\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6875\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6874\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7008\n",
      "100/100 [==============================] - 0s 166us/sample - loss: 0.6874\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.6872\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7013\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.6872\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6871\n",
      "100/100 [==============================] - 0s 120us/sample - loss: 0.7013\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6871\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6869\n",
      "100/100 [==============================] - 0s 130us/sample - loss: 0.7016\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6869\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.6867\n",
      "100/100 [==============================] - 0s 120us/sample - loss: 0.7017\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.6867\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.6865\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7020\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6865\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6863\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7022\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6863\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6861\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.7024\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6861\n",
      "100/100 [==============================] - 0s 81us/sample - loss: 0.6858\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7027\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.6858\n",
      "100/100 [==============================] - 0s 150us/sample - loss: 0.6855\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7029\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.6855\n",
      "100/100 [==============================] - 0s 120us/sample - loss: 0.6852\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.7032\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.6852\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6848\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.7035\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.6848\n",
      "100/100 [==============================] - 0s 139us/sample - loss: 0.6844\n",
      "100/100 [==============================] - 0s 140us/sample - loss: 0.7038\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.6844\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6840\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.7042\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6840\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6834\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.7046\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6834\n",
      "100/100 [==============================] - 0s 110us/sample - loss: 0.6829\n",
      "100/100 [==============================] - 0s 130us/sample - loss: 0.7050\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6829\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6823\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.7054\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6823\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6816\n",
      "100/100 [==============================] - 0s 120us/sample - loss: 0.7059\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6816\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6808\n",
      "100/100 [==============================] - 0s 140us/sample - loss: 0.7064\n",
      "100/100 [==============================] - 0s 49us/sample - loss: 0.6808\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6799\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.7069\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6799\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.6789\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7075\n",
      "100/100 [==============================] - 0s 89us/sample - loss: 0.6789\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6779\n",
      "100/100 [==============================] - 0s 110us/sample - loss: 0.7082\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6779\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6767\n",
      "100/100 [==============================] - 0s 150us/sample - loss: 0.7089\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.6767\n",
      "100/100 [==============================] - 0s 69us/sample - loss: 0.6754\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.7096\n",
      "100/100 [==============================] - 0s 110us/sample - loss: 0.6754\n",
      "100/100 [==============================] - 0s 120us/sample - loss: 0.6739\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7105\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6739\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.6723\n",
      "100/100 [==============================] - 0s 111us/sample - loss: 0.7114\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.6723\n",
      "100/100 [==============================] - 0s 120us/sample - loss: 0.6706\n",
      "100/100 [==============================] - 0s 150us/sample - loss: 0.7124\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6706\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6686\n",
      "100/100 [==============================] - 0s 109us/sample - loss: 0.7134\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 109us/sample - loss: 0.6665\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7146\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6665\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6642\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.7158\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6642\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6616\n",
      "100/100 [==============================] - 0s 69us/sample - loss: 0.7172\n",
      "100/100 [==============================] - 0s 19us/sample - loss: 0.6616\n",
      "100/100 [==============================] - 0s 79us/sample - loss: 0.6588\n",
      "100/100 [==============================] - 0s 59us/sample - loss: 0.7186\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.6588\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6558\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7202\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.6558\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.6526\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.7219\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6526\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6491\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7237\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6491\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6453\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7256\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.6453\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.6413\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.7277\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6413\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.6370\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7299\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.6370\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6324\n",
      "100/100 [==============================] - 0s 70us/sample - loss: 0.7322\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6324\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6276\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.7347\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.6276\n",
      "100/100 [==============================] - 0s 69us/sample - loss: 0.6226\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.7373\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6226\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6173\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.7400\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6173\n",
      "100/100 [==============================] - 0s 69us/sample - loss: 0.6118\n",
      "100/100 [==============================] - 0s 69us/sample - loss: 0.7429\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6118\n",
      "100/100 [==============================] - 0s 89us/sample - loss: 0.6062\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.7459\n",
      "100/100 [==============================] - 0s 30us/sample - loss: 0.6062\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.6003\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.7491\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.6003\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.5943\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7523\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.5943\n",
      "100/100 [==============================] - 0s 40us/sample - loss: 0.5882\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.7557\n",
      "100/100 [==============================] - 0s 19us/sample - loss: 0.5882\n",
      "100/100 [==============================] - 0s 80us/sample - loss: 0.5820\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.803 - 0s 70us/sample - loss: 0.7591\n",
      "100/100 [==============================] - 0s 20us/sample - loss: 0.5820\n",
      "100/100 [==============================] - 0s 90us/sample - loss: 0.5757\n",
      "100/100 [==============================] - 0s 49us/sample - loss: 0.7627\n",
      "100/100 [==============================] - 0s 60us/sample - loss: 0.5757\n",
      "100/100 [==============================] - 0s 49us/sample - loss: 0.5694\n",
      "100/100 [==============================] - 0s 99us/sample - loss: 0.7663\n",
      "100/100 [==============================] - 0s 19us/sample - loss: 0.5694\n",
      "100/100 [==============================] - 0s 100us/sample - loss: 0.5631\n",
      "100/100 [==============================] - 0s 50us/sample - loss: 0.7700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.69060527, 0.68932193, 0.68869015, 0.68832957, 0.68810101,\n",
       "        0.68793002, 0.68778486, 0.68764922, 0.68751498, 0.68737709,\n",
       "        0.68723229, 0.68707786, 0.68691129, 0.68673019, 0.68653207,\n",
       "        0.68631406, 0.68607331, 0.68580675, 0.68551095, 0.68518211,\n",
       "        0.68481621, 0.68440866, 0.68395456, 0.68344845, 0.68288449,\n",
       "        0.68225621, 0.68155641, 0.68077761, 0.67991131, 0.67894853,\n",
       "        0.67787968, 0.67669438, 0.67538172, 0.67393006, 0.6723275 ,\n",
       "        0.67056133, 0.66861902, 0.66648761, 0.66415449, 0.66160725,\n",
       "        0.65883419, 0.65582479, 0.65256976, 0.64906157, 0.6452949 ,\n",
       "        0.64126683, 0.63697745, 0.63242995, 0.62763061, 0.62258934,\n",
       "        0.61731929, 0.61183694, 0.60616173, 0.60031554, 0.59432253,\n",
       "        0.58820834, 0.58199958, 0.57572305, 0.56940554, 0.56307281],\n",
       "       [0.70860526, 0.69563783, 0.70347963, 0.69788031, 0.70170673,\n",
       "        0.6992882 , 0.70115821, 0.70016274, 0.70111642, 0.70076867,\n",
       "        0.70130542, 0.70125797, 0.70161313, 0.70171129, 0.70199576,\n",
       "        0.70217239, 0.70243982, 0.70266836, 0.70294716, 0.70321986,\n",
       "        0.70352719, 0.70384567, 0.70419459, 0.70456532, 0.70496814,\n",
       "        0.70540116, 0.70587063, 0.70637829, 0.70692901, 0.70752608,\n",
       "        0.70817442, 0.70887823, 0.70964267, 0.71047285, 0.71137423,\n",
       "        0.71235238, 0.71341333, 0.71456295, 0.71580753, 0.71715294,\n",
       "        0.71860527, 0.72017038, 0.72185374, 0.72366041, 0.72559495,\n",
       "        0.72766104, 0.72986164, 0.73219887, 0.73467347, 0.73728539,\n",
       "        0.7400329 , 0.7429134 , 0.74592303, 0.74905629, 0.7523068 ,\n",
       "        0.75566729, 0.75912905, 0.76268284, 0.76631862, 0.77002575]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_vs_epoch(x_train, y_train, x_test, y_test, W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.array([1.8,3.4,1.5,0.6,3.5])\n",
    "C_new=np.zeros(7)\n",
    "for i in range(5):\n",
    "    C_new[i+1]=C[i]\n",
    "C=C_new\n",
    "C_new=np.zeros(31)\n",
    "C_new\n",
    "C_index=copy.copy(C_new)\n",
    "for i in range (6):\n",
    "    for j in range(5):\n",
    "        C_new[i*5+j]=C[i]+j*(C[i+1]-C[i])/5\n",
    "        C_index[i*5+j]=0.2*j+i\n",
    "C_new[-1]=0\n",
    "C_index[-1]=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 1.8, 3.4, 1.5, 0.6, 3.5, 0. ])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.36, 0.72, 1.08, 1.44, 1.8 , 2.12, 2.44, 2.76, 3.08, 3.4 ,\n",
       "       3.02, 2.64, 2.26, 1.88, 1.5 , 1.32, 1.14, 0.96, 0.78, 0.6 , 1.18,\n",
       "       1.76, 2.34, 2.92, 3.5 , 2.8 , 2.1 , 1.4 , 0.7 , 0.  ])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4,\n",
       "       2.6, 2.8, 3. , 3.2, 3.4, 3.6, 3.8, 4. , 4.2, 4.4, 4.6, 4.8, 5. ,\n",
       "       5.2, 5.4, 5.6, 5.8, 6. ])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 1s 18ms/sample - loss: -18.3828\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 0s 416us/sample - loss: -19.1665\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 0s 567us/sample - loss: -19.1665\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 0s 382us/sample - loss: -19.1665\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 0s 350us/sample - loss: -19.1665\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 0s 433us/sample - loss: -19.1665\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 0s 366us/sample - loss: -19.1665\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 0s 350us/sample - loss: -19.1665\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 0s 349us/sample - loss: -19.1665\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 0s 333us/sample - loss: -19.1665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2140fcba8>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "model.add(Dense(6, activation='relu', input_dim=1))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.0))\n",
    "model.fit(C_index, C_new, epochs=10, batch_size=5)#,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import copy\n",
    "\n",
    "def polyline_model(C):\n",
    "    model = Sequential()\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    C_new=np.zeros(7)\n",
    "    for i in range(5):\n",
    "        C_new[i+1]=C[i]\n",
    "    C=C_new\n",
    "    C_new=np.zeros(31)\n",
    "    C_new\n",
    "    C_index=copy.copy(C_new)\n",
    "    for i in range (6):\n",
    "        for j in range(5):\n",
    "            C_new[i*5+j]=C[i]+j*(C[i+1]-C[i])/5\n",
    "            C_index[i*5+j]=0.2*j+i\n",
    "    C_new[-1]=0\n",
    "    C_index[-1]=6\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Dense(6, activation='relu', input_dim=1))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=SGD(lr=1.0))\n",
    "    model.fit(C_index, C_new, epochs=20, batch_size=60)#,validation_data=(x_test, y_test))\n",
    "\n",
    "#     model.layers[1].set_weights([W1,np.zeros(20)])\n",
    "#     model.layers[2].set_weights([W2,np.zeros(1)])\n",
    "    \n",
    "    #### END YOUR CODE HERE ####\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60/60 [==============================] - 1s 17ms/sample - loss: 7.5452\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 32us/sample - loss: 6.9105\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 0s 34us/sample - loss: 6.8739\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8561\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8454\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8382\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 34us/sample - loss: 6.8330\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8292\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8261\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8237\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 50us/sample - loss: 6.8217\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 50us/sample - loss: 6.8200\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8186\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 50us/sample - loss: 6.8174\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 33us/sample - loss: 6.8164\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 49us/sample - loss: 6.8154\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 50us/sample - loss: 6.8146\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 50us/sample - loss: 6.8139\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 34us/sample - loss: 6.8132\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 17us/sample - loss: 6.8126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1f220245588>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polyline_model(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/60 [==============================] - 1s 20ms/sample - loss: 7.2531\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 0s 99us/sample - loss: 6.9412\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 0s 134us/sample - loss: 6.9412\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 0s 134us/sample - loss: 6.9412\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 0s 116us/sample - loss: 6.9412\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 0s 200us/sample - loss: 6.9412\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 0s 132us/sample - loss: 6.9412\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 0s 101us/sample - loss: 6.9412\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 0s 134us/sample - loss: 6.9412\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 0s 99us/sample - loss: 6.9412\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 0s 99us/sample - loss: 6.9412\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 0s 134us/sample - loss: 6.9412\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 0s 116us/sample - loss: 6.9412\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 0s 116us/sample - loss: 6.9412\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 0s 101us/sample - loss: 6.9412\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 0s 116us/sample - loss: 6.9412\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 0s 116us/sample - loss: 6.9412\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 0s 150us/sample - loss: 6.9412\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 0s 233us/sample - loss: 6.9412\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 0s 167us/sample - loss: 6.9412\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 0s 150us/sample - loss: 6.9412\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 0s 200us/sample - loss: 6.9412\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 0s 150us/sample - loss: 6.9412\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 0s 133us/sample - loss: 6.9412\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 0s 117us/sample - loss: 6.9412\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 0s 183us/sample - loss: 6.9412\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 0s 83us/sample - loss: 6.9412\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 0s 100us/sample - loss: 6.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2231c5ac8>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import copy\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "C_new=np.zeros(7)\n",
    "C_new[6]=6\n",
    "for i in range(5):\n",
    "    C_new[i+1]=C[i]\n",
    "C=C_new\n",
    "C_new=np.zeros(60)\n",
    "C_new\n",
    "C_index=copy.copy(C_new)\n",
    "for i in range (6):\n",
    "    for j in range(10):\n",
    "        C_new[i*10+j]=C[i]+j*(C[i+1]-C[i])/10\n",
    "        C_index[i*10+j]=0.1*j+i\n",
    "\n",
    "\n",
    "model.add(Dense(6, activation='relu', input_dim=1))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.0))\n",
    "model.fit(C_index, C_new, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = model.predict(C_index)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10,10), dpi=80)\n",
    "# plt.axis('off')\n",
    "\n",
    "\n",
    "# plt.imshow(p_test, cmap='Greys')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02184597],\n",
       "       [0.02478904],\n",
       "       [0.02820319],\n",
       "       [0.03216067],\n",
       "       [0.0367412 ],\n",
       "       [0.04203168],\n",
       "       [0.04812461],\n",
       "       [0.05511647],\n",
       "       [0.06310531],\n",
       "       [0.07218698],\n",
       "       [0.08245134],\n",
       "       [0.09397718],\n",
       "       [0.10682771],\n",
       "       [0.12104532],\n",
       "       [0.13664687],\n",
       "       [0.15362027],\n",
       "       [0.17192218],\n",
       "       [0.19147709],\n",
       "       [0.21217844],\n",
       "       [0.23389205],\n",
       "       [0.25645977],\n",
       "       [0.279706  ],\n",
       "       [0.30344415],\n",
       "       [0.32748276],\n",
       "       [0.35163328],\n",
       "       [0.37571463],\n",
       "       [0.39955914],\n",
       "       [0.42301556],\n",
       "       [0.44595188],\n",
       "       [0.46825647],\n",
       "       [0.48983833],\n",
       "       [0.5106266 ],\n",
       "       [0.5305693 ],\n",
       "       [0.5496317 ],\n",
       "       [0.56779426],\n",
       "       [0.58505034],\n",
       "       [0.60140485],\n",
       "       [0.6168715 ],\n",
       "       [0.63147116],\n",
       "       [0.64523077],\n",
       "       [0.658181  ],\n",
       "       [0.6703557 ],\n",
       "       [0.68179065],\n",
       "       [0.6925226 ],\n",
       "       [0.70258856],\n",
       "       [0.71202534],\n",
       "       [0.7208692 ],\n",
       "       [0.72915506],\n",
       "       [0.73691726],\n",
       "       [0.7441881 ],\n",
       "       [0.7509984 ],\n",
       "       [0.75737786],\n",
       "       [0.76335436],\n",
       "       [0.76895404],\n",
       "       [0.7742015 ],\n",
       "       [0.7791201 ],\n",
       "       [0.7837312 ],\n",
       "       [0.7880554 ],\n",
       "       [0.7921116 ],\n",
       "       [0.7959173 ]], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.array([1.8,3.4,1.5,0.6,3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def polyline_model(C):\n",
    "    model = Sequential()\n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    model.add(Dense(6, activation='relu', input_dim=1))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    W=np.array([[0.],[0.],[0.],[0.],[0.],[0.]])\n",
    "\n",
    "    C_new=np.zeros(7)\n",
    "    C_new[6]=0\n",
    "    for i in range(5):\n",
    "        C_new[i+1]=C[i]\n",
    "    C=C_new\n",
    "    \n",
    "    for i in range(6):\n",
    "        W[i][0]=C[i+1]-C[i]-(C[i]-C[i-1])\n",
    "\n",
    "    \n",
    "    model.layers[0].set_weights([np.array([[1,1,1,1,1,1]]),np.array([0,-1,-2,-3,-4,-5])])\n",
    "    model.layers[1].set_weights([W,np.array([0])])\n",
    "\n",
    "    #### END YOUR CODE HERE ####\n",
    "    return model,W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1f2278f6a58>,\n",
       " array([[ 1.8],\n",
       "        [-0.2],\n",
       "        [-3.5],\n",
       "        [ 1. ],\n",
       "        [ 3.8],\n",
       "        [-6.4]]))"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polyline_model(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def loss_vs_epoch(x_train, y_train, x_test, y_test, W1, W2):\n",
    "    loss = np.zeros((2,60))\n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Reshape((196,), input_shape=(14,14)))\n",
    "    model.add(Dense(units=20, activation='sigmoid'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.0))\n",
    "    model.layers[1].set_weights([W1,np.zeros(20)])\n",
    "    model.layers[2].set_weights([W2,np.zeros(1)])\n",
    "\n",
    "    for i in range(60):\n",
    "        rec=model.fit(x_train, y_train, epochs=1, batch_size=100)\n",
    "        loss[0][i] = model.evaluate(x_train, y_train)\n",
    "        loss[1][i] = model.evaluate(x_test, y_test)\n",
    "    \n",
    "    #### END YOUR CODE HERE ####\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def loss_vs_epoch(x_train, y_train, x_test, y_test, W1, W2):\n",
    "    loss = np.zeros((2,60))\n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Reshape((196,), input_shape=(14,14)))\n",
    "    model.add(Dense(units=20, activation='sigmoid'))\n",
    "    model.add(Dense(units=1, activation='sigmoid',kernel_regularizer=l2(0.1)))   \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.0))\n",
    "    model.layers[1].set_weights([W1,np.zeros(20)])\n",
    "    model.layers[2].set_weights([W2,np.zeros(1)])\n",
    "\n",
    "    for i in range(60):\n",
    "        rec=model.fit(x_train, y_train, epochs=1, batch_size=100)\n",
    "        loss[0][i] = model.evaluate(x_train, y_train)\n",
    "        loss[1][i] = model.evaluate(x_test, y_test)\n",
    "\n",
    "    #### END YOUR CODE HERE ####\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def polyline_model(C):\n",
    "    model = Sequential()\n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    model.add(Dense(6, activation='relu', input_dim=1))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    W=np.array([[0.],[0.],[0.],[0.],[0.],[0.]])\n",
    "\n",
    "    C_new=np.zeros(7)\n",
    "    C_new[6]=0\n",
    "    for i in range(5):\n",
    "        C_new[i+1]=C[i]\n",
    "    C=C_new\n",
    "    \n",
    "    for i in range(6):\n",
    "        W[i][0]=C[i+1]-C[i]-(C[i]-C[i-1])\n",
    "\n",
    "    \n",
    "    model.layers[0].set_weights([np.array([[1,1,1,1,1,1]]),np.array([0,-1,-2,-3,-4,-5])])\n",
    "    model.layers[1].set_weights([W,np.array([0])])\n",
    "\n",
    "    #### END YOUR CODE HERE ####\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
